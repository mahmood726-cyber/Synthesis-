<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>GRADE: The Art of Judging Evidence</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,400;0,600;0,700;1,400;1,600&family=Source+Sans+Pro:wght@300;400;600;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --navy: #1E2761;
      --deep-navy: #141B3D;
      --gold: #D4AF37;
      --cream: #FAF8F5;
      --light-cream: #FFFFFF;
      --text: #2D3748;
      --light-text: #718096;
      --red: #C53030;
      --green: #22c55e;
      --teal: #0f766e;
      --purple: #7c3aed;
      --orange: #f59e0b;
      --transition: 0.5s cubic-bezier(0.4, 0, 0.2, 1);
    }

    * { margin: 0; padding: 0; box-sizing: border-box; }
    html, body {
      height: 100%;
      font-family: 'Source Sans Pro', sans-serif;
      background: var(--deep-navy);
      color: var(--cream);
    }

    .course-container { display: flex; height: 100vh; overflow: hidden; }

    /* Skip Link for Accessibility */
    .skip-link {
      position: absolute;
      top: -40px;
      left: 0;
      background: var(--gold);
      color: var(--navy);
      padding: 8px 16px;
      z-index: 10000;
      text-decoration: none;
      font-weight: 600;
    }
    .skip-link:focus { top: 0; }

    /* ====== SIDEBAR ====== */
    .sidebar {
      width: 300px;
      background: linear-gradient(180deg, var(--deep-navy) 0%, #0a0f1a 100%);
      border-right: 1px solid rgba(212,175,55,0.2);
      display: flex;
      flex-direction: column;
      flex-shrink: 0;
    }

    .sidebar-header {
      padding: 1.25rem;
      border-bottom: 1px solid rgba(212,175,55,0.2);
      text-align: center;
    }
    .sidebar-header h1 {
      font-family: 'Cormorant Garamond', Georgia, serif;
      font-size: 1.3rem;
      font-weight: 700;
      color: var(--gold);
    }
    .sidebar-header p {
      font-size: 0.75rem;
      color: rgba(255,255,255,0.6);
    }

    /* Gamification Stats Panel */
    .stats-panel {
      padding: 1rem;
      background: rgba(212,175,55,0.1);
      border-bottom: 1px solid rgba(212,175,55,0.2);
    }
    .level-display {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-bottom: 0.75rem;
    }
    .level-badge {
      width: 48px;
      height: 48px;
      background: linear-gradient(135deg, var(--gold), #f0c040);
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 1.5rem;
      box-shadow: 0 2px 8px rgba(212,175,55,0.4);
    }
    .level-info { flex: 1; }
    .level-title { font-weight: 700; font-size: 0.9rem; color: var(--gold); }
    .level-subtitle { font-size: 0.7rem; color: rgba(255,255,255,0.6); }
    .xp-bar {
      height: 8px;
      background: rgba(255,255,255,0.1);
      border-radius: 4px;
      overflow: hidden;
      margin-top: 0.5rem;
    }
    .xp-fill {
      height: 100%;
      background: linear-gradient(90deg, var(--gold), var(--orange));
      border-radius: 4px;
      transition: width 0.5s ease;
    }
    .xp-text {
      font-size: 0.65rem;
      color: rgba(255,255,255,0.5);
      text-align: right;
      margin-top: 0.25rem;
    }
    .quick-stats {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 0.5rem;
      margin-top: 0.75rem;
    }
    .quick-stat {
      background: rgba(0,0,0,0.2);
      padding: 0.5rem;
      border-radius: 6px;
      text-align: center;
    }
    .quick-stat-value {
      font-size: 1.1rem;
      font-weight: 700;
      color: var(--gold);
    }
    .quick-stat-label {
      font-size: 0.6rem;
      color: rgba(255,255,255,0.5);
    }

    /* Course Progress */
    .course-progress {
      padding: 0.75rem 1rem;
      border-bottom: 1px solid rgba(212,175,55,0.2);
      background: rgba(0,0,0,0.2);
    }
    .course-progress-label {
      display: flex;
      justify-content: space-between;
      font-size: 0.7rem;
      color: rgba(255,255,255,0.6);
      margin-bottom: 0.35rem;
    }
    .course-progress-bar {
      height: 6px;
      background: rgba(255,255,255,0.1);
      border-radius: 3px;
      overflow: hidden;
    }
    .course-progress-fill {
      height: 100%;
      background: linear-gradient(90deg, var(--green), #22d3ee);
      border-radius: 3px;
      transition: width 0.5s ease;
    }

    .module-list { flex: 1; overflow-y: auto; padding: 0.5rem 0; }
    .module-item {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      padding: 0.6rem 1rem;
      cursor: pointer;
      transition: all 0.2s;
      border-left: 3px solid transparent;
    }
    .module-item:hover { background: rgba(212,175,55,0.1); }
    .module-item.active { background: rgba(212,175,55,0.15); border-left-color: var(--gold); }
    .module-item.completed .module-number { background: var(--green); color: white; }
    .module-item.locked { opacity: 0.5; cursor: not-allowed; }
    .module-number {
      width: 26px;
      height: 26px;
      border-radius: 50%;
      background: rgba(255,255,255,0.1);
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 0.7rem;
      font-weight: 700;
      flex-shrink: 0;
    }
    .module-item.active .module-number { background: var(--gold); color: var(--navy); }
    .module-info { flex: 1; min-width: 0; }
    .module-title { font-size: 0.8rem; font-weight: 600; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }
    .module-subtitle { font-size: 0.65rem; color: rgba(255,255,255,0.5); }
    .module-xp { font-size: 0.6rem; color: var(--gold); }
    .module-time { font-size: 0.55rem; color: rgba(255,255,255,0.4); display: flex; align-items: center; gap: 0.25rem; margin-top: 0.15rem; }

    /* Badges Panel */
    .badges-panel {
      padding: 1rem;
      border-top: 1px solid rgba(212,175,55,0.2);
    }
    .badges-title { font-size: 0.75rem; font-weight: 600; color: var(--gold); margin-bottom: 0.5rem; }
    .badges-grid { display: flex; flex-wrap: wrap; gap: 0.35rem; }
    .badge-item {
      width: 32px;
      height: 32px;
      background: rgba(255,255,255,0.05);
      border: 2px solid rgba(255,255,255,0.1);
      border-radius: 6px;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 0.9rem;
      opacity: 0.3;
      transition: all 0.3s;
      cursor: help;
      position: relative;
      padding: 0;
      color: inherit;
    }
    .badge-item.earned { opacity: 1; background: rgba(212,175,55,0.2); border-color: var(--gold); }
    .badge-item:hover::after {
      content: attr(title);
      position: absolute;
      bottom: 100%;
      left: 50%;
      transform: translateX(-50%);
      background: var(--navy);
      color: white;
      padding: 0.25rem 0.5rem;
      border-radius: 4px;
      font-size: 0.6rem;
      white-space: nowrap;
      z-index: 100;
    }

    /* ====== MAIN CONTENT ====== */
    .main-content { flex: 1; display: flex; flex-direction: column; overflow: hidden; }
    .content-header {
      padding: 0.75rem 1.5rem;
      background: rgba(0,0,0,0.3);
      border-bottom: 1px solid rgba(212,175,55,0.2);
      display: flex;
      align-items: center;
      justify-content: space-between;
    }
    .breadcrumb { font-size: 0.85rem; color: rgba(255,255,255,0.6); }
    .breadcrumb span { color: var(--gold); }
    .header-actions { display: flex; gap: 0.5rem; align-items: center; }
    .streak-display {
      display: flex;
      align-items: center;
      gap: 0.35rem;
      background: rgba(239,68,68,0.2);
      border: 1px solid rgba(239,68,68,0.4);
      padding: 0.35rem 0.75rem;
      border-radius: 20px;
      font-size: 0.8rem;
    }
    .streak-display.active { background: rgba(249,115,22,0.2); border-color: var(--orange); }
    .streak-flame { font-size: 1rem; }
    .streak-count { font-weight: 700; color: var(--orange); }

    .btn {
      padding: 0.5rem 1rem;
      border: 1px solid rgba(212,175,55,0.3);
      border-radius: 6px;
      background: rgba(255,255,255,0.05);
      color: var(--cream);
      font-size: 0.8rem;
      cursor: pointer;
      transition: all 0.2s;
    }
    .btn:hover { background: rgba(212,175,55,0.2); border-color: var(--gold); }
    .btn.primary { background: var(--gold); color: var(--navy); border-color: var(--gold); font-weight: 600; }

    .content-body { flex: 1; overflow: hidden; display: flex; }

    /* ====== SLIDES ====== */
    .slides-container { flex: 1; position: relative; overflow: hidden; }
    .module-slides { display: none; width: 100%; height: 100%; }
    .module-slides.active { display: block; }

    .slide {
      position: absolute;
      top: 0; left: 0;
      width: 100%; height: 100%;
      display: flex;
      flex-direction: column;
      justify-content: flex-start;
      align-items: center;
      padding: 2rem 2rem 4rem;
      opacity: 0;
      visibility: hidden;
      transition: opacity var(--transition), visibility var(--transition);
      overflow-y: auto;
    }
    .slide.active { opacity: 1; visibility: visible; }
    .slide.dark { background: var(--deep-navy); }
    .slide.light { background: var(--cream); color: var(--text); }
    .slide.black { background: #000; }
    .slide.red-bg { background: linear-gradient(135deg, #7f1d1d, #450a0a); }
    .slide.green-bg { background: linear-gradient(135deg, #14532d, #052e16); }
    .slide.purple-bg { background: linear-gradient(135deg, #4c1d95, #2e1065); }
    .slide.orange-bg { background: linear-gradient(135deg, #9a3412, #431407); }

    .slide-progress-bar {
      position: absolute;
      top: 0;
      left: 0;
      height: 3px;
      background: linear-gradient(90deg, var(--gold), var(--orange));
      transition: width 0.3s ease;
      z-index: 50;
    }

    .serif { font-family: 'Cormorant Garamond', Georgia, serif; }
    .title {
      font-family: 'Cormorant Garamond', Georgia, serif;
      font-size: clamp(1.8rem, 4vw, 3rem);
      font-weight: 700;
      margin-bottom: 1rem;
      line-height: 1.2;
      text-align: center;
    }
    .title.gold { color: var(--gold); }
    .title.navy { color: var(--navy); }
    .subtitle {
      font-size: clamp(0.9rem, 2vw, 1.3rem);
      color: rgba(255,255,255,0.7);
      margin-bottom: 1.5rem;
      text-align: center;
    }
    .slide.light .subtitle { color: var(--light-text); }

    .big-number {
      font-family: 'Cormorant Garamond', Georgia, serif;
      font-size: clamp(4rem, 12vw, 8rem);
      font-weight: 700;
      line-height: 1;
    }
    .big-number.gold { color: var(--gold); }
    .big-number.red { color: var(--red); }

    .refrain {
      font-family: 'Cormorant Garamond', Georgia, serif;
      font-size: clamp(1.3rem, 3vw, 2rem);
      font-style: italic;
      color: var(--gold);
      text-align: center;
      max-width: 700px;
    }

    .content { display: flex; flex-direction: column; align-items: center; width: 100%; max-width: 850px; }

    /* Cards */
    .card {
      background: var(--light-cream);
      border: 1px solid var(--gold);
      padding: 1.25rem 1.5rem;
      margin: 0.5rem 0;
      max-width: 750px;
      width: 100%;
      border-radius: 8px;
    }
    .card.navy-bg { background: var(--navy); color: var(--cream); }
    .card.red-border { border: 2px solid var(--red); }
    .card.green-border { border: 2px solid var(--green); }
    .card-label {
      font-size: 0.7rem;
      font-weight: 700;
      color: var(--gold);
      letter-spacing: 0.05em;
      margin-bottom: 0.4rem;
      text-transform: uppercase;
    }
    .card-text { font-size: 0.95rem; line-height: 1.6; }
    .card.navy-bg .card-text { color: var(--cream); }

    /* Story box */
    .story-box {
      background: linear-gradient(135deg, rgba(197,48,48,0.15), rgba(30,39,97,0.2));
      border-left: 4px solid var(--red);
      padding: 1.25rem;
      margin: 0.75rem 0;
      max-width: 750px;
      width: 100%;
      border-radius: 0 8px 8px 0;
    }
    .story-box.success { background: linear-gradient(135deg, rgba(34,197,94,0.15), rgba(30,39,97,0.2)); border-left-color: var(--green); }
    .story-box.warning { background: linear-gradient(135deg, rgba(245,158,11,0.15), rgba(30,39,97,0.2)); border-left-color: var(--orange); }
    .story-label {
      font-size: 0.7rem;
      font-weight: 700;
      letter-spacing: 0.1em;
      color: var(--red);
      margin-bottom: 0.4rem;
      text-transform: uppercase;
    }
    .story-box.success .story-label { color: var(--green); }
    .story-box.warning .story-label { color: var(--orange); }
    .story-text { font-size: 0.95rem; line-height: 1.6; }

    /* Epic Story Box */
    .story-box.epic {
      background: linear-gradient(135deg, rgba(20,27,61,0.95), rgba(0,0,0,0.9));
      border-left: 4px solid var(--gold);
      border-top: 1px solid rgba(212,175,55,0.3);
      border-right: 1px solid rgba(212,175,55,0.2);
      border-bottom: 1px solid rgba(212,175,55,0.2);
      padding: 1.5rem;
      position: relative;
    }
    .story-box.epic::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      height: 2px;
      background: linear-gradient(90deg, var(--gold), transparent);
    }
    .story-box.epic .story-label { color: var(--gold); font-size: 0.65rem; letter-spacing: 0.15em; }
    .story-box.epic .story-text { font-family: 'Cormorant Garamond', Georgia, serif; font-size: 1.05rem; line-height: 1.7; }

    /* GRADE Certainty Colors */
    .grade-high { background: linear-gradient(135deg, rgba(34,197,94,0.2), rgba(30,39,97,0.2)); border-color: var(--green); }
    .grade-moderate { background: linear-gradient(135deg, rgba(59,130,246,0.2), rgba(30,39,97,0.2)); border-color: #3b82f6; }
    .grade-low { background: linear-gradient(135deg, rgba(245,158,11,0.2), rgba(30,39,97,0.2)); border-color: var(--orange); }
    .grade-very-low { background: linear-gradient(135deg, rgba(197,48,48,0.2), rgba(30,39,97,0.2)); border-color: var(--red); }

    .grade-badge {
      display: inline-block;
      padding: 0.25rem 0.75rem;
      border-radius: 4px;
      font-size: 0.75rem;
      font-weight: 700;
      text-transform: uppercase;
      letter-spacing: 0.05em;
    }
    .grade-badge.high { background: var(--green); color: white; }
    .grade-badge.moderate { background: #3b82f6; color: white; }
    .grade-badge.low { background: var(--orange); color: white; }
    .grade-badge.very-low { background: var(--red); color: white; }

    /* Stats */
    .stats-grid {
      display: flex;
      gap: 0.75rem;
      margin: 1rem 0;
      flex-wrap: wrap;
      justify-content: center;
    }
    .stat-box {
      background: var(--navy);
      color: var(--cream);
      padding: 1rem 1.25rem;
      text-align: center;
      min-width: 120px;
      border-radius: 8px;
    }
    .stat-box.light { background: var(--light-cream); color: var(--navy); border: 1px solid var(--gold); }
    .stat-value {
      font-family: 'Cormorant Garamond', serif;
      font-size: clamp(1.3rem, 2.5vw, 2rem);
      font-weight: 700;
    }
    .stat-value.gold { color: var(--gold); }
    .stat-value.red { color: var(--red); }
    .stat-value.green { color: var(--green); }
    .stat-label { font-size: 0.75rem; opacity: 0.8; margin-top: 0.2rem; }

    /* Timeline */
    .timeline { display: flex; flex-direction: column; gap: 0.4rem; width: 100%; max-width: 650px; }
    .timeline-item {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      padding: 0.6rem 0.75rem;
      background: rgba(255,255,255,0.05);
      border-radius: 6px;
      border-left: 4px solid var(--gold);
    }
    .timeline-days { font-weight: 700; font-size: 0.8rem; min-width: 70px; color: var(--gold); }
    .timeline-content { flex: 1; }
    .timeline-title { font-weight: 600; font-size: 0.9rem; }
    .timeline-desc { font-size: 0.75rem; opacity: 0.7; }

    /* Checklist */
    .checklist { display: flex; flex-direction: column; gap: 0.4rem; width: 100%; max-width: 650px; }
    .checklist-item {
      display: flex;
      align-items: flex-start;
      gap: 0.6rem;
      padding: 0.6rem 0.75rem;
      background: rgba(255,255,255,0.05);
      border-radius: 6px;
      border: 1px solid rgba(255,255,255,0.1);
    }
    .check-icon {
      width: 22px;
      height: 22px;
      background: var(--gold);
      color: var(--navy);
      border-radius: 4px;
      display: flex;
      align-items: center;
      justify-content: center;
      font-weight: 700;
      font-size: 0.75rem;
      flex-shrink: 0;
    }
    .checklist-text { font-size: 0.9rem; line-height: 1.4; }

    /* Lessons */
    .lesson-item { display: flex; align-items: flex-start; gap: 0.75rem; margin: 0.5rem 0; max-width: 700px; }
    .lesson-number {
      background: var(--gold);
      color: var(--navy);
      width: 28px;
      height: 28px;
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-weight: 700;
      font-size: 0.85rem;
      flex-shrink: 0;
    }
    .lesson-content h4 { font-size: 1rem; font-weight: 700; margin-bottom: 0.2rem; }
    .lesson-content p { font-size: 0.85rem; opacity: 0.8; }

    /* ====== DECISION TREE ====== */
    .decision-tree {
      width: 100%;
      max-width: 700px;
      background: linear-gradient(135deg, rgba(124,58,237,0.15), rgba(30,39,97,0.25));
      border: 2px solid var(--purple);
      border-radius: 12px;
      padding: 1.5rem;
      margin: 0.75rem 0;
    }
    .decision-header {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-bottom: 1rem;
    }
    .decision-icon {
      width: 40px;
      height: 40px;
      background: var(--purple);
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 1.25rem;
    }
    .decision-title {
      font-family: 'Cormorant Garamond', serif;
      font-size: 1.2rem;
      font-weight: 700;
    }
    .decision-xp {
      margin-left: auto;
      background: rgba(212,175,55,0.2);
      padding: 0.25rem 0.75rem;
      border-radius: 12px;
      font-size: 0.75rem;
      color: var(--gold);
      font-weight: 600;
    }
    .decision-scenario {
      background: rgba(0,0,0,0.2);
      padding: 1rem;
      border-radius: 8px;
      margin-bottom: 1rem;
      font-size: 0.95rem;
      line-height: 1.6;
    }
    .decision-question {
      font-weight: 600;
      color: var(--gold);
      margin-bottom: 0.75rem;
      font-size: 1rem;
    }
    .decision-options { display: flex; flex-direction: column; gap: 0.5rem; }
    .decision-option {
      display: flex;
      align-items: flex-start;
      gap: 0.75rem;
      padding: 0.75rem 1rem;
      background: rgba(255,255,255,0.05);
      border: 2px solid rgba(255,255,255,0.15);
      border-radius: 8px;
      cursor: pointer;
      transition: all 0.2s;
    }
    .decision-option:hover { background: rgba(124,58,237,0.2); border-color: var(--purple); }
    .decision-option.selected { background: rgba(124,58,237,0.3); border-color: var(--purple); }
    .decision-option.correct { background: rgba(34,197,94,0.2); border-color: var(--green); }
    .decision-option.incorrect { background: rgba(239,68,68,0.15); border-color: var(--red); }
    .decision-option.disabled { pointer-events: none; opacity: 0.7; }
    .decision-option:focus-visible { outline: 3px solid var(--gold); outline-offset: 2px; }
    .option-letter {
      width: 24px;
      height: 24px;
      background: rgba(255,255,255,0.1);
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-weight: 700;
      font-size: 0.8rem;
      flex-shrink: 0;
    }
    .decision-option.correct .option-letter { background: var(--green); color: white; }
    .decision-option.incorrect .option-letter { background: var(--red); color: white; }
    .option-text { font-size: 0.9rem; }
    .decision-feedback {
      margin-top: 1rem;
      padding: 1rem;
      border-radius: 8px;
      display: none;
    }
    .decision-feedback.show { display: block; }
    .decision-feedback.correct { background: rgba(34,197,94,0.2); border: 1px solid var(--green); }
    .decision-feedback.incorrect { background: rgba(239,68,68,0.15); border: 1px solid var(--red); }
    .feedback-title { font-weight: 700; margin-bottom: 0.5rem; display: flex; align-items: center; gap: 0.5rem; }
    .feedback-text { font-size: 0.9rem; line-height: 1.5; }

    /* ====== QUIZ ====== */
    .quiz-container { width: 100%; max-width: 650px; }
    .quiz-question {
      font-family: 'Cormorant Garamond', serif;
      font-size: 1.2rem;
      margin-bottom: 1.25rem;
      text-align: center;
    }
    .quiz-options { display: flex; flex-direction: column; gap: 0.5rem; }
    .quiz-option {
      padding: 0.75rem 1rem;
      background: rgba(255,255,255,0.05);
      border: 2px solid rgba(255,255,255,0.2);
      border-radius: 8px;
      cursor: pointer;
      transition: all 0.2s;
      text-align: left;
      font-size: 0.9rem;
    }
    .quiz-option:hover { background: rgba(212,175,55,0.15); border-color: var(--gold); }
    .quiz-option.correct { background: rgba(34,197,94,0.2); border-color: var(--green); }
    .quiz-option.incorrect { background: rgba(239,68,68,0.15); border-color: var(--red); }
    .quiz-option.disabled { pointer-events: none; }
    .quiz-option:focus-visible { outline: 3px solid var(--gold); outline-offset: 2px; }
    .quiz-feedback {
      margin-top: 1rem;
      padding: 1rem;
      border-radius: 8px;
      display: none;
      font-size: 0.9rem;
    }
    .quiz-feedback.show { display: block; }
    .quiz-feedback.correct { background: rgba(34,197,94,0.2); border: 1px solid var(--green); }
    .quiz-feedback.incorrect { background: rgba(239,68,68,0.15); border: 1px solid var(--red); }

    /* Learning Objectives */
    .objectives-box {
      background: linear-gradient(135deg, rgba(59,130,246,0.15), rgba(30,39,97,0.25));
      border: 2px solid #3b82f6;
      border-radius: 12px;
      padding: 1.25rem;
      margin: 0.75rem 0;
      max-width: 700px;
      width: 100%;
    }
    .objectives-header {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-bottom: 0.75rem;
    }
    .objectives-icon {
      width: 36px;
      height: 36px;
      background: #3b82f6;
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 1.1rem;
    }
    .objectives-title {
      font-family: 'Cormorant Garamond', serif;
      font-size: 1.1rem;
      font-weight: 700;
      color: #60a5fa;
    }
    .objectives-list { list-style: none; padding: 0; }
    .objectives-list li {
      display: flex;
      align-items: flex-start;
      gap: 0.5rem;
      padding: 0.4rem 0;
      font-size: 0.9rem;
    }
    .objectives-list li::before { content: '‚óã'; color: #3b82f6; font-weight: bold; }

    /* Animation */
    .slide.active .animate { animation: fadeUp 0.5s ease-out forwards; }
    .slide.active .animate.delay-1 { animation-delay: 0.1s; }
    .slide.active .animate.delay-2 { animation-delay: 0.2s; }
    .slide.active .animate.delay-3 { animation-delay: 0.3s; }
    @keyframes fadeUp {
      from { opacity: 0; transform: translateY(15px); }
      to { opacity: 1; transform: translateY(0); }
    }
    .animate { opacity: 0; }

    /* XP Popup */
    .xp-popup {
      position: fixed;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%) scale(0);
      background: linear-gradient(135deg, var(--gold), #f0c040);
      color: var(--navy);
      padding: 1.5rem 2.5rem;
      border-radius: 12px;
      font-family: 'Cormorant Garamond', serif;
      font-size: 2rem;
      font-weight: 700;
      z-index: 1000;
      box-shadow: 0 10px 40px rgba(0,0,0,0.5);
      transition: transform 0.3s ease;
    }
    .xp-popup.show { transform: translate(-50%, -50%) scale(1); }
    .xp-popup .xp-amount { font-size: 3rem; display: block; }

    /* Badge Earned Modal */
    .badge-modal {
      position: fixed;
      top: 0; left: 0; right: 0; bottom: 0;
      background: rgba(0,0,0,0.85);
      display: none;
      justify-content: center;
      align-items: center;
      z-index: 1001;
    }
    .badge-modal.show { display: flex; }
    .badge-modal-content {
      background: linear-gradient(135deg, var(--deep-navy), #0a0f1a);
      border: 2px solid var(--gold);
      border-radius: 16px;
      padding: 2rem;
      text-align: center;
      max-width: 350px;
    }
    .badge-modal-icon { font-size: 4rem; margin-bottom: 1rem; }
    .badge-modal-title {
      font-family: 'Cormorant Garamond', serif;
      font-size: 1.5rem;
      color: var(--gold);
      margin-bottom: 0.5rem;
    }
    .badge-modal-desc { font-size: 0.9rem; opacity: 0.8; margin-bottom: 1rem; }
    .badge-modal-close {
      padding: 0.5rem 1.5rem;
      background: var(--gold);
      color: var(--navy);
      border: none;
      border-radius: 6px;
      font-weight: 600;
      cursor: pointer;
    }

    /* Confetti */
    .confetti-container {
      position: fixed;
      top: 0; left: 0;
      width: 100%; height: 100%;
      pointer-events: none;
      z-index: 999;
      overflow: hidden;
    }
    .confetti {
      position: absolute;
      width: 10px; height: 10px;
      opacity: 0;
      animation: confetti-fall 3s ease-out forwards;
    }
    @keyframes confetti-fall {
      0% { transform: translateY(-100px) rotate(0deg); opacity: 1; }
      100% { transform: translateY(100vh) rotate(720deg); opacity: 0; }
    }
    @media (prefers-reduced-motion: reduce) {
      .confetti { animation: none; display: none; }
      .xp-popup { transition: none; }
      .slide { transition: none; }
    }

    /* Nav */
    .slide-nav {
      position: absolute;
      bottom: 0; left: 0; right: 0;
      display: flex;
      justify-content: space-between;
      align-items: flex-end;
      padding: 0.75rem 1rem 1.25rem;
      z-index: 100;
      pointer-events: none;
    }
    .nav-btn {
      background: rgba(0,0,0,0.5);
      border: 1px solid rgba(255,255,255,0.2);
      color: white;
      font-size: 1.1rem;
      cursor: pointer;
      opacity: 0.6;
      transition: all 0.2s;
      padding: 0.4rem 0.6rem;
      border-radius: 6px;
      pointer-events: auto;
    }
    .nav-btn:hover { opacity: 1; background: rgba(0,0,0,0.7); border-color: var(--gold); }
    .nav-btn:disabled { opacity: 0.2; cursor: not-allowed; }
    .nav-btn:focus-visible { outline: 3px solid var(--gold); outline-offset: 2px; }
    .nav-center { display: flex; flex-direction: column; align-items: center; gap: 0.2rem; pointer-events: auto; }
    .slide-counter { font-size: 0.7rem; opacity: 0.5; }
    .progress-dots { display: flex; gap: 3px; flex-wrap: wrap; max-width: 220px; justify-content: center; }
    .progress-dot {
      width: 44px;
      height: 44px;
      display: flex;
      align-items: center;
      justify-content: center;
      cursor: pointer;
      background: none;
      border: none;
      padding: 0;
    }
    .progress-dot::after {
      content: '';
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background: var(--text-muted, rgba(255,255,255,0.25));
      transition: all 0.2s ease;
    }
    .progress-dot.active::after,
    .progress-dot.completed::after {
      background: var(--gold);
      width: 14px;
      height: 14px;
    }

    /* Mobile */
    @media (max-width: 768px) {
      .sidebar {
        position: fixed;
        left: -300px;
        top: 0;
        bottom: 0;
        z-index: 1000;
        transition: left 0.3s ease;
      }
      .sidebar.open { left: 0; }
      .sidebar-toggle {
        display: flex;
        position: fixed;
        top: 10px;
        left: 10px;
        z-index: 1001;
        width: 40px;
        height: 40px;
        background: var(--gold);
        color: var(--navy);
        border: none;
        border-radius: 8px;
        align-items: center;
        justify-content: center;
        font-size: 1.2rem;
        cursor: pointer;
      }
      .sidebar-overlay {
        display: none;
        position: fixed;
        top: 0; left: 0; right: 0; bottom: 0;
        background: rgba(0,0,0,0.5);
        z-index: 999;
      }
      .sidebar-overlay.show { display: block; }
    }
    @media (min-width: 769px) {
      .sidebar-toggle { display: none; }
      .sidebar-overlay { display: none !important; }
    }

    /* Combo Counter */
    .combo-display {
      position: fixed;
      top: 80px;
      right: 20px;
      background: linear-gradient(135deg, var(--orange), #ef4444);
      color: white;
      padding: 0.5rem 1rem;
      border-radius: 20px;
      font-weight: 700;
      font-size: 0.9rem;
      z-index: 500;
      transform: scale(0);
      transition: transform 0.3s ease;
      box-shadow: 0 4px 15px rgba(239,68,68,0.4);
    }
    .combo-display.show { transform: scale(1); }
  </style>
</head>
<body>
  <a href="#main-content" class="skip-link">Skip to main content</a>

  <div class="course-container">
    <!-- Sidebar -->
    <nav class="sidebar" role="navigation" aria-label="Course navigation">
      <div class="sidebar-header">
        <a href="index.html" style="display:block;font-size:0.7rem;color:rgba(212,175,55,0.7);text-decoration:none;margin-bottom:0.5rem;" onmouseover="this.style.color='#D4AF37'" onmouseout="this.style.color='rgba(212,175,55,0.7)'">&larr; Course Library</a>
        <h1>GRADE Mastery</h1>
        <p>The Art of Judging Evidence</p>
      </div>

      <div class="stats-panel">
        <div class="level-display">
          <div class="level-badge" id="levelIcon">üìä</div>
          <div class="level-info">
            <div class="level-title" id="levelTitle">Novice Appraiser</div>
            <div class="level-subtitle">Level <span id="levelNum">1</span></div>
          </div>
        </div>
        <div class="xp-bar"><div class="xp-fill" id="xpFill" style="width: 0%"></div></div>
        <div class="xp-text"><span id="xpCurrent">0</span> / <span id="xpNeeded">100</span> XP</div>
        <div class="quick-stats">
          <div class="quick-stat">
            <div class="quick-stat-value" id="correctCount">0</div>
            <div class="quick-stat-label">Correct</div>
          </div>
          <div class="quick-stat">
            <div class="quick-stat-value" id="decisionCount">0</div>
            <div class="quick-stat-label">Decisions</div>
          </div>
          <div class="quick-stat">
            <div class="quick-stat-value" id="gradesCount">0</div>
            <div class="quick-stat-label">Graded</div>
          </div>
        </div>
      </div>

      <div class="course-progress">
        <div class="course-progress-label">
          <span>Course Progress</span>
          <span id="courseProgressPct">0%</span>
        </div>
        <div class="course-progress-bar">
          <div class="course-progress-fill" id="courseProgressFill" style="width: 0%"></div>
        </div>
      </div>

      <div class="module-list" id="moduleList"></div>

      <div class="badges-panel">
        <div class="badges-title">Achievements</div>
        <div class="badges-grid" id="badgesGrid"></div>
      </div>
    </nav>

    <!-- Main Content -->
    <main class="main-content" id="main-content">
      <header class="content-header">
        <div class="breadcrumb">GRADE Mastery / <span id="currentModule">Module 1</span></div>
        <div class="header-actions">
          <div class="streak-display" id="streakDisplay">
            <span class="streak-flame">üî•</span>
            <span class="streak-count" id="streakCount">0</span>
            <span>day streak</span>
          </div>
          <button class="btn" onclick="resetProgress()">Reset</button>
        </div>
      </header>

      <div class="content-body">
        <div class="slides-container" id="slidesContainer"></div>
      </div>
    </main>
  </div>

  <!-- XP Popup -->
  <div class="xp-popup" id="xpPopup" role="status" aria-live="polite" aria-atomic="true">
    <span class="xp-amount" id="xpAmount">+25</span>
    XP
  </div>

  <!-- Badge Modal -->
  <div class="badge-modal" id="badgeModal" role="dialog" aria-modal="true" aria-labelledby="badgeModalTitle">
    <div class="badge-modal-content">
      <div class="badge-modal-icon" id="badgeModalIcon">üèÜ</div>
      <div class="badge-modal-title" id="badgeModalTitle">Badge Earned!</div>
      <div class="badge-modal-desc" id="badgeModalDesc">You unlocked a new achievement</div>
      <button class="badge-modal-close" onclick="closeBadgeModal()">Awesome!</button>
    </div>
  </div>

  <!-- Confetti Container -->
  <div class="confetti-container" id="confettiContainer"></div>

  <!-- Combo Counter -->
  <div class="combo-display" id="comboDisplay" role="status" aria-live="polite">üî• <span id="comboCount">0</span>x Combo!</div>

  <!-- Mobile Sidebar Toggle -->
  <button class="sidebar-toggle" onclick="toggleSidebar()" aria-label="Toggle menu" aria-expanded="false">‚ò∞</button>
  <div class="sidebar-overlay" id="sidebarOverlay" onclick="toggleSidebar()"></div>

<script>
// ===== GAMIFICATION STATE =====
const DEFAULT_STATE = {
  xp: 0,
  level: 1,
  streak: 0,
  lastActive: null,
  correctAnswers: 0,
  decisionsCompleted: 0,
  gradesAssigned: 0,
  badges: [],
  completedModules: [],
  answeredQuestions: {},
  consecutiveCorrect: 0
};

function safeGetStorage(key, fallback) {
  try {
    const item = localStorage.getItem(key);
    return item ? JSON.parse(item) : fallback;
  } catch (e) {
    return fallback;
  }
}

function safeSetStorage(key, value) {
  try {
    localStorage.setItem(key, JSON.stringify(value));
  } catch (e) {}
}

// Migration from old storage key to versioned key
(function migrateStorage() {
  try {
    const oldData = localStorage.getItem('gradeGame');
    if (oldData && !localStorage.getItem('gradeGame_v1')) {
      localStorage.setItem('gradeGame_v1', oldData);
      localStorage.removeItem('gradeGame');
    }
  } catch (e) {}
})();

let gameState = safeGetStorage('gradeGame_v1', DEFAULT_STATE);

const LEVELS = [
  { level: 1, title: "Novice Appraiser", icon: "üìä", xp: 0 },
  { level: 2, title: "Bias Spotter", icon: "üîç", xp: 100 },
  { level: 3, title: "Certainty Seeker", icon: "‚öñÔ∏è", xp: 250 },
  { level: 4, title: "Evidence Evaluator", icon: "üìã", xp: 450 },
  { level: 5, title: "GRADE Apprentice", icon: "üéì", xp: 700 },
  { level: 6, title: "Certainty Guardian", icon: "üõ°Ô∏è", xp: 1000 },
  { level: 7, title: "Evidence Master", icon: "üèÜ", xp: 1400 },
  { level: 8, title: "GRADE Sage", icon: "üëë", xp: 1900 }
];

const BADGES = [
  { id: "first_grade", icon: "üìä", name: "First Rating", desc: "Assign your first GRADE rating" },
  { id: "bias_hunter", icon: "üîç", name: "Bias Hunter", desc: "Complete the Risk of Bias module" },
  { id: "precision_master", icon: "üéØ", name: "Precision Master", desc: "Complete the Imprecision module" },
  { id: "consistency_king", icon: "üîó", name: "Consistency King", desc: "Complete the Inconsistency module" },
  { id: "directness_detective", icon: "üé™", name: "Directness Detective", desc: "Complete the Indirectness module" },
  { id: "publication_sleuth", icon: "üì∞", name: "Publication Sleuth", desc: "Complete Publication Bias module" },
  { id: "upgrader", icon: "‚¨ÜÔ∏è", name: "The Upgrader", desc: "Learn all upgrading factors" },
  { id: "perfect_5", icon: "‚≠ê", name: "Five Star", desc: "Get 5 correct answers in a row" },
  { id: "decision_maker", icon: "‚öñÔ∏è", name: "Decision Maker", desc: "Complete 15 decision trees" },
  { id: "finisher", icon: "üèÅ", name: "Course Complete", desc: "Finish the entire course" },
  { id: "streak_3", icon: "üî•", name: "On Fire", desc: "Maintain a 3-day streak" },
  { id: "streak_7", icon: "üí´", name: "Weekly Warrior", desc: "Maintain a 7-day streak" },
  { id: "grade_master", icon: "üëë", name: "GRADE Master", desc: "Complete the Final Assessment" }
];

// ===== MODULES DATA =====
const MODULES = [
  {
    id: 1, title: "Why Certainty Matters", subtitle: "When confidence kills", xp: 150,
    slides: [
      { type: "title", theme: "dark", content: { title: "Why Certainty Matters", subtitle: "When overconfidence in evidence kills patients" }},
      { type: "objectives", theme: "dark", content: {
        objectives: [
          "Understand why evidence certainty determines patient outcomes",
          "Learn from guidelines that killed because certainty was misjudged",
          "Recognize the difference between 'evidence exists' and 'evidence is trustworthy'",
          "Understand what GRADE provides that traditional reviews don't"
        ],
        tip: "This module shows why getting certainty wrong has real consequences."
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Beta-Blocker Catastrophe", title: "Guidelines Based on a Liar",
        text: "For two decades, clinical guidelines worldwide recommended <strong>beta-blockers before non-cardiac surgery</strong> to prevent heart attacks. The recommendation was rated 'strong'‚Äîhigh certainty of benefit. Millions of patients received the drugs before operations. The evidence came largely from one man: <strong>Don Poldermans</strong>, who contributed 15 studies showing dramatic benefits.",
        followup: "In 2011, Poldermans was found guilty of research misconduct. His data was fabricated. When his studies were removed from meta-analyses, the math reversed: beta-blockers didn't prevent heart attacks‚Äîthey caused them. Researchers calculated the cost: following guidelines based on this fraudulent 'high certainty' evidence may have <strong>killed 800,000 patients</strong> over a decade. The certainty rating was wrong. The patients paid with their lives."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "‚ö†Ô∏è", title: "Scenario: The Strong Recommendation",
        xp: 25,
        scenario: "A guideline panel rates evidence as 'high certainty' and issues a 'strong recommendation' for a preventive treatment. You notice that 60% of the evidence comes from a single research group whose findings have never been independently replicated.",
        question: "What concern should you raise about the certainty rating?",
        options: [
          { letter: "A", text: "None‚Äîpeer-reviewed studies are trustworthy", correct: false },
          { letter: "B", text: "The certainty should be downgraded for risk of bias and lack of replication", correct: true },
          { letter: "C", text: "Request more studies from the same group to confirm", correct: false },
          { letter: "D", text: "The strong recommendation compensates for any uncertainty", correct: false }
        ],
        feedback: {
          correct: "Correct! Poldermans' unreplicated findings were rated 'high certainty' despite depending on one source. GRADE requires downgrading when evidence depends heavily on potentially biased sources that lack independent confirmation.",
          incorrect: "800,000 deaths resulted from trusting unreplicated evidence from a single source. High certainty requires independent replication. Concentration in one research group is a critical risk of bias."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The Hormone Reversal", title: "Two Million Women Betrayed",
        text: "For forty years, doctors prescribed <strong>hormone replacement therapy (HRT)</strong> to prevent heart disease in menopausal women. The evidence seemed overwhelming: observational studies showed <strong>50% fewer heart attacks</strong>. Experts were certain. Guidelines were strong. Two million American women took HRT specifically for heart protection.",
        followup: "In 2002, the Women's Health Initiative RCT delivered a verdict that shattered decades of certainty. HRT didn't prevent heart disease‚Äîit <strong>increased heart attacks by 29%</strong>, strokes by 41%, and breast cancer by 26%. The observational evidence had been confounded: healthy women chose HRT, creating an illusion of benefit. The certainty had been based on the wrong type of evidence. Two million women had taken a drug that harmed them, prescribed by doctors who were certain they were helping."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìä", title: "Scenario: Observational vs. RCT",
        xp: 25,
        scenario: "Twenty large observational studies show a treatment reduces cardiovascular deaths by 45%. Two small RCTs show no effect. A guideline panel must rate the certainty of evidence for benefit.",
        question: "What certainty rating is appropriate?",
        options: [
          { letter: "A", text: "High‚Äî20 studies outweigh 2 studies", correct: false },
          { letter: "B", text: "Moderate‚Äîmixed evidence requires caution", correct: false },
          { letter: "C", text: "Low‚ÄîRCTs trump observational studies; confounding likely explains the discrepancy", correct: true },
          { letter: "D", text: "Very Low‚Äîno conclusion is possible", correct: false }
        ],
        feedback: {
          correct: "Correct! The HRT disaster taught us this exact lesson. Observational studies showing 50% benefit were trumped by RCTs showing harm. When RCTs contradict observational data, confounding is the likely explanation. GRADE starts RCTs at high, observational at low.",
          incorrect: "HRT observational studies showed 50% benefit; the RCT showed 29% harm. The discrepancy was confounding‚Äîhealthy women chose HRT. RCT evidence, even from smaller studies, provides higher certainty than observational evidence for causal questions."
        }
      }},
      { type: "stats", theme: "dark", content: {
        title: "The Cost of Wrong Certainty",
        stats: [
          { value: "800K", label: "Deaths from Poldermans fraud" },
          { value: "2M", label: "Women harmed by HRT" },
          { value: "50K", label: "CAST drug deaths" },
          { value: "83K", label: "Avandia heart attacks" }
        ],
        caption: "Each disaster involved guidelines that were 'certain' based on flawed evidence assessment."
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Intensive Glucose Disaster", title: "ACCORD: When Tight Control Killed",
        text: "Diabetes doctors were certain: if high blood sugar caused complications, then tighter glucose control must prevent them. The logic seemed irrefutable. Guidelines recommended <strong>aggressive targets</strong>. The ACCORD trial enrolled 10,251 patients to prove what everyone already 'knew'‚Äîintensive glucose control saves lives.",
        followup: "In February 2008, the trial was stopped early. Not for benefit‚Äî<strong>for harm</strong>. Intensive glucose control increased deaths by 22%. The certainty had been based on biological plausibility, not actual outcome data. The surrogate outcome (glucose levels) improved while the patient-important outcome (survival) worsened. 257 excess deaths occurred in the intensive group. The guidelines had been confident. The confidence was misplaced."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üéØ", title: "Scenario: The Logical Treatment",
        xp: 25,
        scenario: "A treatment dramatically improves a biomarker (lab value) that is known to cause disease. No trials have yet measured actual clinical outcomes like death or hospitalization. A colleague argues the certainty of benefit is 'high' because the mechanism is well-understood.",
        question: "What certainty rating should apply?",
        options: [
          { letter: "A", text: "High‚Äîclear mechanism implies clear benefit", correct: false },
          { letter: "B", text: "Moderate‚Äîmechanism is supportive but not definitive", correct: false },
          { letter: "C", text: "Low or Very Low‚Äîsurrogate outcomes don't guarantee patient-important benefits", correct: true },
          { letter: "D", text: "Cannot rate until trials are complete", correct: false }
        ],
        feedback: {
          correct: "Correct! ACCORD, CAST, and countless other disasters followed the 'logical mechanism' fallacy. Improving surrogates can worsen outcomes. GRADE requires downgrading for indirectness when only surrogate outcomes are measured.",
          incorrect: "ACCORD intensive therapy improved glucose perfectly while killing patients. CAST drugs fixed ECGs while tripling deaths. Biological plausibility is not clinical evidence. Patient-important outcomes are required for high certainty."
        }
      }},
      { type: "concept", theme: "dark", content: {
        title: "What GRADE Provides",
        subtitle: "A systematic framework for certainty",
        text: "GRADE (Grading of Recommendations Assessment, Development and Evaluation) provides a structured, transparent method for rating how much confidence we should place in evidence. It separates 'evidence exists' from 'evidence is trustworthy.'",
        highlight: "GRADE asks: 'How confident are we that the true effect lies close to the estimated effect?'"
      }},
      { type: "story", theme: "dark", content: {
        label: "The Arthroscopy Revelation", title: "A Billion-Dollar Placebo",
        text: "Knee arthroscopy for osteoarthritis was one of the most common orthopedic procedures in the world. <strong>700,000 Americans</strong> underwent it annually at a cost of $3 billion. Surgeons were certain it worked‚Äîthey saw patients improve. Insurance covered it. Guidelines recommended it.",
        followup: "Then came the sham surgery trials. Patients were randomized to real arthroscopy or fake surgery (incisions but no actual procedure). The result: <strong>no difference</strong>. Patients improved equally whether they received real surgery or placebo. Decades of certainty, billions of dollars, millions of surgeries‚Äîall based on the placebo effect that surgeons couldn't see. The certainty had been based on uncontrolled observations. Controlled trials revealed the truth."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üî¨", title: "Scenario: The Experienced Surgeon",
        xp: 25,
        scenario: "A senior surgeon argues that a procedure 'definitely works' because she has performed it 2,000 times and 'seen patients improve.' No randomized trials exist. She wants the guideline to rate certainty as 'moderate' based on clinical experience.",
        question: "What certainty rating is appropriate?",
        options: [
          { letter: "A", text: "Moderate‚Äî2,000 cases is substantial experience", correct: false },
          { letter: "B", text: "Low‚Äîuncontrolled observations can't distinguish treatment effect from placebo", correct: true },
          { letter: "C", text: "High‚Äîexpert experience is the highest form of evidence", correct: false },
          { letter: "D", text: "Very Low‚Äîanecdotes are not evidence", correct: false }
        ],
        feedback: {
          correct: "Correct! Knee arthroscopy surgeons 'saw' patients improve for decades‚Äîthen sham surgery trials showed it was placebo. Expert experience cannot detect placebo effects. GRADE rates uncontrolled observations as very low certainty.",
          incorrect: "Surgeons performed 700,000 arthroscopies yearly, 'seeing' benefit‚Äîthat was entirely placebo. Clinical experience, no matter how extensive, cannot distinguish treatment effects from natural improvement or placebo. RCTs are essential."
        }
      }},
      { type: "refrain", theme: "black", content: { text: "Certainty is not about how much evidence exists. It's about how much we should trust it." }},
      { type: "quiz", theme: "dark", content: {
        question: "Why did the HRT observational studies mislead doctors for 40 years?",
        options: [
          { text: "The studies were fabricated", correct: false },
          { text: "Healthy women were more likely to choose HRT, creating confounding", correct: true },
          { text: "The RCTs used different doses", correct: false },
          { text: "Heart disease definitions changed over time", correct: false }
        ],
        xp: 20,
        feedback: {
          correct: "Exactly. Confounding by indication‚Äîhealthy women chose HRT‚Äîcreated an illusion of benefit. This is why GRADE rates observational studies lower than RCTs for causal questions.",
          incorrect: "The studies were real, but confounded. Healthy women were more likely to take HRT, making it appear beneficial. RCTs randomize away this confounding, revealing the true effect."
        }
      }}
    ]
  },
  {
    id: 2, title: "The GRADE Framework", subtitle: "From high to very low", xp: 120,
    slides: [
      { type: "title", theme: "dark", content: { title: "The GRADE Framework", subtitle: "Four levels of certainty, five reasons to doubt" }},
      { type: "objectives", theme: "dark", content: {
        objectives: [
          "Learn the four GRADE certainty levels and what they mean",
          "Understand starting points: RCTs vs observational studies",
          "Know the five domains for downgrading certainty",
          "Know the three factors for upgrading observational evidence"
        ],
        tip: "This module provides the framework you'll apply throughout the course."
      }},
      { type: "stats", theme: "dark", content: {
        title: "The Four Certainty Levels",
        stats: [
          { value: "‚äï‚äï‚äï‚äï", label: "HIGH" },
          { value: "‚äï‚äï‚äï‚óã", label: "MODERATE" },
          { value: "‚äï‚äï‚óã‚óã", label: "LOW" },
          { value: "‚äï‚óã‚óã‚óã", label: "VERY LOW" }
        ],
        caption: "Each level tells decision-makers how much trust to place in the evidence."
      }},
      { type: "concept", theme: "dark", content: {
        title: "What Each Level Means",
        subtitle: "Practical interpretation",
        checkpoints: [
          { day: "HIGH", event: "Very confident the true effect is close to the estimate" },
          { day: "MODERATE", event: "Moderately confident; true effect likely close but may differ" },
          { day: "LOW", event: "Limited confidence; true effect may be substantially different" },
          { day: "VERY LOW", event: "Very little confidence; true effect likely substantially different" }
        ]
      }},
      { type: "story", theme: "dark", content: {
        label: "The Starting Point Principle", title: "Why RCTs Begin Higher",
        text: "In 1747, James Lind conducted what many consider the first controlled trial. Twelve sailors with scurvy were divided into six pairs, each receiving a different treatment. <strong>Only the two who received citrus fruits recovered.</strong> This simple comparison‚Äîtreated vs. untreated under similar conditions‚Äîrevealed truth that centuries of observation had missed.",
        followup: "Randomization is the great equalizer. It balances known and unknown confounders between groups. When patients are randomized, differences in outcomes can be attributed to the treatment, not to differences between patients. This is why GRADE starts RCT evidence at <strong>HIGH</strong> certainty and observational evidence at <strong>LOW</strong>‚Äîthe study design itself determines how much confounding might distort results."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üé≤", title: "Scenario: The Starting Point",
        xp: 25,
        scenario: "You're rating evidence for a new drug. You have three RCTs (total n=1,200) and five cohort studies (total n=45,000). A colleague argues the cohort studies should start at the same certainty level because they have more patients.",
        question: "What is the correct starting point for each evidence type?",
        options: [
          { letter: "A", text: "Both start at HIGH‚Äîsample size determines certainty", correct: false },
          { letter: "B", text: "RCTs start HIGH, cohorts start LOW‚Äîdesign determines starting point", correct: true },
          { letter: "C", text: "Cohorts start MODERATE because of large sample size", correct: false },
          { letter: "D", text: "Both start at MODERATE and adjust from there", correct: false }
        ],
        feedback: {
          correct: "Correct! GRADE starting points are based on study design, not sample size. RCTs start HIGH because randomization controls confounding. Observational studies start LOW because confounding cannot be eliminated, regardless of size.",
          incorrect: "Sample size affects precision, not confounding. The HRT cohort studies had millions of patients but were still wrong because of confounding. RCTs start HIGH; observational studies start LOW."
        }
      }},
      { type: "checklist", theme: "dark", content: {
        title: "Five Domains for Downgrading",
        items: [
          "Risk of Bias ‚Äî Are the studies at high risk of bias?",
          "Inconsistency ‚Äî Do results vary unexplainably across studies?",
          "Indirectness ‚Äî Does evidence directly address our question?",
          "Imprecision ‚Äî Is the effect estimate precise enough?",
          "Publication Bias ‚Äî Are studies missing from the evidence base?"
        ]
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Tamiflu Saga", title: "When All Five Domains Failed",
        text: "In 2009, governments stockpiled Tamiflu for pandemic flu. The UK spent <strong>¬£473 million</strong>. The evidence looked solid: published trials showed reduced complications. But Cochrane reviewers noticed problems in every GRADE domain.",
        followup: "<strong>Risk of bias:</strong> Trials were industry-funded with unclear methods. <strong>Inconsistency:</strong> Results varied between published and unpublished studies. <strong>Indirectness:</strong> 'Complications' were poorly defined. <strong>Imprecision:</strong> Confidence intervals were wide. <strong>Publication bias:</strong> 60% of patient data was never published. When Roche finally released the raw data after five years, the truth emerged: Tamiflu reduced symptom duration by less than a day and had no proven effect on hospitalizations or complications. Half a billion pounds spent on evidence that should have been rated VERY LOW."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìâ", title: "Scenario: The Multi-Domain Problem",
        xp: 25,
        scenario: "You're rating evidence from 5 RCTs. You find: unclear allocation concealment (risk of bias), I¬≤ = 75% with no explanation (inconsistency), and half the trials never published (publication bias). The trials themselves are precise and direct.",
        question: "How many levels should you downgrade from HIGH?",
        options: [
          { letter: "A", text: "One level‚Äîmultiple minor concerns equal one downgrade", correct: false },
          { letter: "B", text: "Two levels‚Äîserious concerns in two domains", correct: false },
          { letter: "C", text: "Three levels‚Äîserious concerns in three domains (to VERY LOW)", correct: true },
          { letter: "D", text: "Cannot downgrade RCTs‚Äîthey're always HIGH", correct: false }
        ],
        feedback: {
          correct: "Correct! Serious concerns in each domain typically warrant one downgrade each. Risk of bias (‚àí1), inconsistency (‚àí1), and publication bias (‚àí1) would take HIGH to VERY LOW. Multiple domains can compound.",
          incorrect: "Each domain with serious concerns typically warrants one downgrade. Three serious domain concerns = three downgrades. HIGH ‚àí 3 = VERY LOW. GRADE is cumulative."
        }
      }},
      { type: "checklist", theme: "dark", content: {
        title: "Three Factors for Upgrading (Observational Only)",
        items: [
          "Large Effect ‚Äî Very large magnitude of effect (RR > 2 or < 0.5)",
          "Dose-Response ‚Äî Clear gradient between exposure and outcome",
          "Confounding ‚Äî All plausible confounding would reduce the effect"
        ]
      }},
      { type: "story", theme: "green-bg", content: {
        label: "The Smoking Evidence", title: "When Observation Earned Certainty",
        text: "In the 1950s, Richard Doll and Austin Bradford Hill published observational studies linking cigarettes to lung cancer. No RCTs existed‚Äîyou cannot randomize people to smoke. Yet the evidence became <strong>HIGH certainty</strong>. Why?",
        followup: "Every upgrading criterion was met. <strong>Large effect:</strong> Smokers had 10-30x higher lung cancer rates‚Äîan effect size that confounding alone cannot explain. <strong>Dose-response:</strong> More cigarettes = more cancer in a clear gradient. <strong>Confounding direction:</strong> Any unmeasured confounder that caused both smoking and cancer would have to be implausibly powerful to explain the effect. When observational evidence meets all three upgrading criteria, it can achieve certainty that rivals RCTs."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "‚¨ÜÔ∏è", title: "Scenario: The Upgrading Question",
        xp: 25,
        scenario: "An observational study shows a treatment increases survival with RR = 1.3 (30% relative increase). The association is statistically significant with narrow confidence intervals. The authors argue this should be upgraded from LOW to MODERATE.",
        question: "Should this evidence be upgraded?",
        options: [
          { letter: "A", text: "Yes‚Äîstatistical significance warrants upgrading", correct: false },
          { letter: "B", text: "Yes‚Äînarrow confidence intervals warrant upgrading", correct: false },
          { letter: "C", text: "No‚ÄîRR 1.3 is not large enough; confounding could easily explain it", correct: true },
          { letter: "D", text: "No‚Äîobservational evidence can never be upgraded", correct: false }
        ],
        feedback: {
          correct: "Correct! RR 1.3 is well within the range that confounding can produce. Large effect upgrading typically requires RR > 2 or < 0.5‚Äîeffects so large that confounding alone is an implausible explanation. Statistical significance and precision are not upgrading criteria.",
          incorrect: "A 30% relative increase (RR 1.3) can easily result from modest confounding. The HRT studies showed RR 0.5 for heart protection‚Äîentirely due to confounding. Large effect upgrading requires effects that confounding cannot plausibly explain (typically RR > 2 or < 0.5)."
        }
      }},
      { type: "concept", theme: "dark", content: {
        title: "The GRADE Mnemonic: RIIPP",
        subtitle: "Remember the 5 downgrading domains",
        text: "<strong style='color:var(--gold);font-size:1.5rem'>R</strong>isk of bias ‚Äî Can we trust the studies?<br><br><strong style='color:var(--gold);font-size:1.5rem'>I</strong>nconsistency ‚Äî Do studies agree?<br><br><strong style='color:var(--gold);font-size:1.5rem'>I</strong>ndirectness ‚Äî Does evidence match our question?<br><br><strong style='color:var(--gold);font-size:1.5rem'>I</strong>mprecision ‚Äî Is the estimate precise?<br><br><strong style='color:var(--gold);font-size:1.5rem'>P</strong>ublication bias ‚Äî Is evidence missing?",
        highlight: "RIIPP ‚Äî Remember: 'RCT evidence can RIIPP apart if these domains have serious problems.'"
      }},
      { type: "quiz", theme: "dark", content: {
        question: "Why do observational studies start at LOW certainty in GRADE?",
        options: [
          { text: "They usually have smaller sample sizes", correct: false },
          { text: "They cannot control for unmeasured confounding", correct: true },
          { text: "They are more often industry-funded", correct: false },
          { text: "They take longer to complete", correct: false }
        ],
        xp: 20,
        feedback: {
          correct: "Exactly. Randomization balances confounders between groups. Without it, differences in outcomes might reflect differences in patients, not treatment effects. HRT studies with millions of patients were still wrong because of confounding.",
          incorrect: "The issue is confounding, not sample size. Observational studies cannot balance unknown confounders the way randomization does. This fundamental limitation determines the starting point."
        }
      }}
    ]
  },
  {
    id: 3, title: "Risk of Bias", subtitle: "When studies lie", xp: 180,
    slides: [
      { type: "title", theme: "purple-bg", content: { title: "Risk of Bias", subtitle: "The first domain: Can we trust these studies?" }},
      { type: "objectives", theme: "dark", content: {
        objectives: [
          "Identify key bias domains in RCTs and observational studies",
          "Recognize how bias inflates or deflates treatment effects",
          "Apply risk of bias assessment to real examples",
          "Decide when to downgrade certainty for bias"
        ],
        tip: "Complete all decision scenarios to earn the Bias Hunter badge!"
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The VIGOR Deception", title: "When Merck Chose the Cutoff",
        text: "In 2000, Merck published the VIGOR trial comparing Vioxx to naproxen. The New England Journal of Medicine reported a 4-fold increase in heart attacks with Vioxx. But the published paper hid something: <strong>three additional heart attacks</strong> occurred in the Vioxx group after the data cutoff date. Merck scientists knew about them.",
        followup: "Those three heart attacks mattered. Including them would have changed the risk ratio from 'concerning' to 'alarming.' Instead, Merck's chosen cutoff date‚Äîa methodological decision made after seeing the data‚Äîobscured the true risk. The NEJM later published an 'expression of concern,' calling the paper 'inaccurate and incomplete.' By then, tens of thousands had suffered heart attacks from a drug whose danger had been hidden by selective reporting. The bias wasn't in the trial design‚Äîit was in how the data was analyzed and presented."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "‚úÇÔ∏è", title: "Scenario: The Moving Cutoff",
        xp: 30,
        scenario: "You're assessing a trial that changed its primary analysis timepoint after the study started‚Äîfrom 12 months to 6 months. The 6-month results favor the treatment. The authors say the change was made because of 'administrative reasons' unrelated to outcomes.",
        question: "How should this affect your risk of bias assessment?",
        options: [
          { letter: "A", text: "No concern‚Äîadministrative changes happen", correct: false },
          { letter: "B", text: "High risk of bias for selective reporting‚Äîtimepoint changes after seeing data are suspicious", correct: true },
          { letter: "C", text: "Low risk if the 12-month data is also reported", correct: false },
          { letter: "D", text: "Unclear risk‚Äîwe cannot know their true motivation", correct: false }
        ],
        feedback: {
          correct: "Correct! Post-hoc changes to analysis plans are high risk for selective reporting. VIGOR's data cutoff was a 'methodological decision' that hid three heart attacks. When timepoints change after data accumulates, assume the worst.",
          incorrect: "Merck's 'methodological' cutoff date hid fatal heart attacks. Post-hoc analysis changes‚Äîespecially when results favor treatment‚Äîare classic selective reporting. GRADE requires skepticism about post-hoc modifications."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The Open-Label Problem", title: "When Knowing Changes Everything",
        text: "The ASCOT-LLA trial compared atorvastatin to placebo for cardiovascular prevention. The primary endpoint was non-fatal MI plus fatal CHD‚Äî<strong>objective outcomes</strong> that shouldn't depend on whether doctors knew which treatment patients received.",
        followup: "But doctors did know. The trial was stopped early when the Data Safety Monitoring Board saw benefit. The problem: some outcomes were adjudicated by committees who were <strong>not blinded</strong> to treatment assignment. Did knowing a patient was on statin affect how borderline MIs were classified? Studies comparing blinded vs. unblinded assessment show systematic differences. Open-label designs can inflate apparent benefits by 10-20% even for 'objective' outcomes. ASCOT-LLA was rated down for risk of bias despite being an RCT."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üëÅÔ∏è", title: "Scenario: The Unblinded Assessors",
        xp: 25,
        scenario: "An RCT of a surgical intervention reports 30% reduction in 'major cardiovascular events.' The trial was necessarily unblinded (patients knew if they had surgery). Outcome assessors were also unblinded. Events were adjudicated by a committee that knew treatment assignments.",
        question: "What is the risk of bias for detection bias?",
        options: [
          { letter: "A", text: "Low‚Äîcardiovascular events are objective", correct: false },
          { letter: "B", text: "High‚Äîunblinded assessment of cardiovascular events inflates effects", correct: true },
          { letter: "C", text: "Unclear‚Äîwe need more information about the committee", correct: false },
          { letter: "D", text: "Not applicable‚Äîsurgical trials cannot be blinded", correct: false }
        ],
        feedback: {
          correct: "Correct! Even 'objective' outcomes like MI involve judgment calls (borderline troponins, ECG interpretations). Studies show unblinded assessment inflates effects 10-20%. Outcome assessors should always be blinded when possible, regardless of intervention blinding.",
          incorrect: "Cardiovascular events require interpretation‚Äîborderline troponin elevations, ECG changes, symptom classification. Unblinded assessors systematically favor the intervention. Outcome assessment should be blinded even when intervention blinding is impossible."
        }
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The ITT Violation", title: "Roche's Missing Patients",
        text: "In Tamiflu trials, Roche excluded patients from analysis after randomization. Some were excluded for 'protocol violations'‚Äîbut the violations often occurred <strong>after</strong> patients had taken the drug and experienced side effects. Patients who developed complications were retroactively deemed 'protocol violators' and removed.",
        followup: "Intention-to-treat (ITT) analysis exists precisely to prevent this. Once randomized, always analyzed‚Äîregardless of what happens afterward. Excluding patients after randomization destroys the balance that randomization created. Roche's selective exclusions removed patients who had adverse outcomes, making Tamiflu appear safer and more effective than it was. The Cochrane review downgraded certainty specifically for these ITT violations."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üö™", title: "Scenario: The Post-Randomization Exclusions",
        xp: 25,
        scenario: "A trial randomizes 500 patients per arm. The paper reports results for 420 vs. 380 patients, with 80 and 120 exclusions respectively. Reasons include: 'withdrew consent,' 'lost to follow-up,' and 'protocol deviation.' More patients were excluded from the control arm.",
        question: "What risk of bias concern does this raise?",
        options: [
          { letter: "A", text: "No concern‚Äîexclusions happen in every trial", correct: false },
          { letter: "B", text: "High risk of attrition bias‚Äîdifferential exclusions destroy randomization balance", correct: true },
          { letter: "C", text: "Low risk if reasons are documented", correct: false },
          { letter: "D", text: "Unclear‚Äîwe need to know why controls were excluded more", correct: false }
        ],
        feedback: {
          correct: "Correct! Differential attrition (120 vs. 80 exclusions) suggests the groups are no longer comparable. If control patients who weren't improving were more likely to drop out, the remaining controls look healthier than they should. This inflates apparent treatment benefit.",
          incorrect: "Differential exclusions (120 vs. 80) break randomization. The remaining groups are no longer balanced. If sicker control patients dropped out, the control group appears artificially healthy. GRADE requires downgrading for substantial differential attrition."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The Registration Gap", title: "The Outcomes That Disappeared",
        text: "A systematic review of trial registration found that <strong>31% of trials</strong> changed their primary outcome between registration and publication. The changes weren't random‚Äîthey systematically favored positive results. Outcomes that showed benefit were promoted to primary; outcomes showing no effect were demoted or disappeared.",
        followup: "Selective outcome reporting is invisible without registration comparison. A trial might register five outcomes, find benefit in one, and publish only that one as the 'primary' outcome. The reader sees a positive trial. The reality: 4 of 5 outcomes showed nothing. PROSPERO registration for systematic reviews and ClinicalTrials.gov for trials exist specifically to combat this. When assessing risk of bias, always compare published outcomes to registered protocols."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìã", title: "Scenario: The Primary Outcome Switch",
        xp: 25,
        scenario: "You compare a published trial to its registration. Registration lists 'cardiovascular death' as primary. The publication lists 'composite of CV death, MI, or stroke' as primary and reports it as significant. Cardiovascular death alone is buried in a table and shows no significant difference.",
        question: "What risk of bias rating is appropriate?",
        options: [
          { letter: "A", text: "Low‚Äîcomposite endpoints are clinically reasonable", correct: false },
          { letter: "B", text: "High‚Äîthis is outcome switching to achieve significance", correct: true },
          { letter: "C", text: "Unclear‚Äîthe authors may have had good reasons", correct: false },
          { letter: "D", text: "Low if the composite was pre-specified in the protocol", correct: false }
        ],
        feedback: {
          correct: "Correct! Switching from a narrow outcome (CV death) to a broader composite (CV death/MI/stroke) after seeing that the narrow outcome failed is textbook outcome switching. The 31% of trials that do this systematically inflate positive results.",
          incorrect: "This is outcome switching‚Äîthe registered primary failed, so a broader composite was promoted. The composite significance may be driven entirely by the less important components. Compare every publication to its registration."
        }
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Avandia Audit", title: "38,000 Heart Attacks Exposed by One Cardiologist",
        text: "In 2007, Dr. Steven Nissen at the Cleveland Clinic did what regulators hadn't: he pooled all available rosiglitazone (Avandia) data. <strong>42 trials, 27,847 patients</strong>. The FDA had approved Avandia based on surrogate outcomes‚Äîit lowered blood sugar beautifully. But Nissen asked a different question: what about hearts?",
        followup: "His meta-analysis revealed the truth: Avandia increased heart attacks by <strong>43%</strong> (OR 1.43, 95% CI 1.03-1.98). GSK had conducted these trials. GSK had the data. But the data on cardiovascular events had been scattered across trials, buried in adverse event tables, never pooled. When Nissen pooled it, the pattern became undeniable. The FDA estimated <strong>83,000 excess heart attacks</strong> between 1999-2007. The bias wasn't in study conduct‚Äîit was in how results were analyzed and presented. Selective synthesis can be as dangerous as selective reporting."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìë", title: "Scenario: The Scattered Safety Data",
        xp: 25,
        scenario: "A drug has been studied in 30 trials for various conditions. Each trial reports a few adverse cardiac events in supplementary tables, but no single trial was powered for cardiac outcomes. A meta-analysis combining cardiac events across all 30 trials shows RR 1.35 (95% CI 1.10-1.65) for heart attacks.",
        question: "How should this post-hoc safety signal affect certainty?",
        options: [
          { letter: "A", text: "Ignore it‚Äîtrials weren't designed for cardiac outcomes", correct: false },
          { letter: "B", text: "Take it seriously‚Äîpooled adverse events can reveal real signals even from post-hoc analysis", correct: true },
          { letter: "C", text: "Dismiss it‚Äîmeta-analysis of adverse events is unreliable", correct: false },
          { letter: "D", text: "Wait for a dedicated cardiac outcome trial before acting", correct: false }
        ],
        feedback: {
          correct: "Correct! Nissen's Avandia analysis was 'post-hoc' pooling of scattered adverse events‚Äîand it revealed 83,000 excess heart attacks. Post-hoc safety signals from meta-analysis can be real. The absence of pre-specified cardiac outcomes doesn't mean cardiac events didn't happen.",
          incorrect: "The Avandia case proved that post-hoc pooling of adverse events can reveal genuine harm. Waiting for a dedicated trial while 83,000 heart attacks accumulated would have been unconscionable. Safety signals from adverse event meta-analysis deserve serious consideration."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The ENHANCE Deception", title: "When Merck Hid What They Found",
        text: "In 2006, Merck completed the ENHANCE trial of Vytorin (ezetimibe + simvastatin) for atherosclerosis. The trial measured carotid artery thickness‚Äîa surrogate for cardiovascular disease. Results were due in early 2007. Merck announced a 'delay' for 'additional analyses.'",
        followup: "The 'additional analyses' took <strong>two years</strong>. When finally released in 2008, ENHANCE showed <strong>no benefit</strong>‚ÄîVytorin didn't reduce artery thickness despite lowering LDL cholesterol. During those two years of concealment, Vytorin sales exceeded <strong>$5 billion</strong>. Congressional investigations revealed internal documents showing Merck knew the results were negative in 2006. The delay wasn't for analysis‚Äîit was for commercial protection. Publication delay is a form of selective reporting: timing becomes a bias tool."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "‚è∞", title: "Scenario: The Suspicious Delay",
        xp: 25,
        scenario: "A highly-anticipated trial of a blockbuster drug was supposed to report results 18 months ago. The company cites 'ongoing analyses' and 'data quality issues.' Meanwhile, the drug continues selling billions annually. Competitor trials have already reported.",
        question: "What should you suspect about the unreported results?",
        options: [
          { letter: "A", text: "Nothing‚Äîdata quality takes time", correct: false },
          { letter: "B", text: "Likely negative or harmful‚Äîcompanies delay bad news, not good news", correct: true },
          { letter: "C", text: "Likely positive‚Äîthey're waiting for regulatory approval", correct: false },
          { letter: "D", text: "Cannot infer anything from publication timing", correct: false }
        ],
        feedback: {
          correct: "Correct! The ENHANCE delay taught us: companies don't delay good news for two years. Unexplained publication delays, especially for products still generating revenue, are strong evidence of unfavorable results. Consider delayed trials as potentially biased against the intervention.",
          incorrect: "Publication timing is informative. Companies accelerate publication of positive results (stock price, marketing) and delay negative ones (continued sales). An 18-month delay for a blockbuster drug strongly suggests the results don't support continued use."
        }
      }},
      { type: "checklist", theme: "dark", content: {
        title: "Risk of Bias Domains (RCTs)",
        items: [
          "Random sequence generation ‚Äî Was randomization truly random?",
          "Allocation concealment ‚Äî Could anyone predict assignments?",
          "Blinding of participants/personnel ‚Äî Did they know treatment?",
          "Blinding of outcome assessment ‚Äî Did assessors know treatment?",
          "Incomplete outcome data ‚Äî Was attrition balanced and explained?",
          "Selective reporting ‚Äî Are registered outcomes all reported?",
          "Other bias ‚Äî Baseline imbalances, early stopping, etc."
        ]
      }},
      { type: "quiz", theme: "dark", content: {
        question: "Why does GRADE downgrade for open-label outcome assessment even when outcomes seem 'objective'?",
        options: [
          { text: "Open-label trials are always lower quality", correct: false },
          { text: "Even objective outcomes involve judgment calls affected by knowledge of treatment", correct: true },
          { text: "Regulations require blinded assessment", correct: false },
          { text: "It's a conservative convention, not based on evidence", correct: false }
        ],
        xp: 20,
        feedback: {
          correct: "Right! Borderline MI, uncertain stroke symptoms, debatable ECG changes‚Äîall involve interpretation. Studies show unblinded assessment systematically favors the known treatment by 10-20% even for 'hard' endpoints.",
          incorrect: "Research shows unblinded assessment inflates effects even for outcomes that seem objective. Borderline cases get interpreted in favor of the expected effect. That's detection bias."
        }
      }}
    ]
  },
  {
    id: 4, title: "Inconsistency", subtitle: "When studies disagree", xp: 160,
    slides: [
      { type: "title", theme: "orange-bg", content: { title: "Inconsistency", subtitle: "The second domain: What do we do when studies disagree?" }},
      { type: "objectives", theme: "dark", content: {
        objectives: [
          "Understand why study results vary and when variation is concerning",
          "Interpret I¬≤ and tau¬≤ statistics for heterogeneity",
          "Distinguish explainable from unexplainable inconsistency",
          "Decide when to downgrade certainty for inconsistency"
        ],
        tip: "Real disagreement between studies tells us something important."
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Vitamin D Contradiction", title: "When 200 Studies Couldn't Agree",
        text: "By 2019, over <strong>200 randomized trials</strong> had studied vitamin D supplementation. The results were chaos. Some showed vitamin D prevented cancer (RR 0.77). Others showed no effect (RR 1.02). Some showed harm (RR 1.15). Meta-analyses pooled them all and proclaimed 'statistically significant benefit.' But the I¬≤ was <strong>78%</strong>‚Äîmassive heterogeneity that the summary estimate concealed.",
        followup: "The largest trial, VITAL (25,871 participants), finally brought clarity. It showed <strong>no benefit</strong> for cancer or cardiovascular disease. The smaller positive studies had methodological problems: inadequate randomization, post-hoc analyses, selected populations. The pooled estimate had hidden a crucial truth: when studies wildly disagree, the average may represent no one's truth. GRADE requires downgrading for this kind of inconsistency, regardless of what the pooled estimate shows."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìä", title: "Scenario: The Heterogeneous Meta-Analysis",
        xp: 25,
        scenario: "A meta-analysis of 15 trials shows pooled benefit (OR 0.75, p<0.001). However, I¬≤ = 82%, tau¬≤ is large, and the prediction interval crosses 1.0. Six trials show harm, nine show benefit. The authors conclude 'significant benefit with moderate heterogeneity.'",
        question: "How should inconsistency affect the certainty rating?",
        options: [
          { letter: "A", text: "No downgrade‚Äîthe pooled estimate is statistically significant", correct: false },
          { letter: "B", text: "Downgrade one level‚ÄîI¬≤ > 75% is substantial heterogeneity", correct: false },
          { letter: "C", text: "Downgrade one or two levels‚Äîprediction interval crossing 1.0 means future studies might show harm", correct: true },
          { letter: "D", text: "Cannot rate‚Äîinconsistent evidence is uninterpretable", correct: false }
        ],
        feedback: {
          correct: "Correct! The prediction interval is key. When it crosses the null, we're saying: 'If we did another study, it might show the opposite effect.' That's profound uncertainty. I¬≤ = 82% with a prediction interval crossing 1.0 warrants serious downgrading.",
          incorrect: "I¬≤ of 82% is severe heterogeneity, but the prediction interval is more informative. It crossing 1.0 means future studies might show harm‚Äîthe pooled 'benefit' may not apply to the next patient. Statistical significance of the pooled estimate doesn't rescue inconsistent evidence."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The Stroke Mystery", title: "Aspirin's Vanishing Benefit",
        text: "In 1988, the ISIS-2 trial proved aspirin reduced death after heart attack by <strong>23%</strong>. The evidence was bulletproof‚Äî17,187 patients, clear benefit, minimal heterogeneity when pooled with similar trials. Aspirin became standard care.",
        followup: "But stroke prevention was different. Meta-analyses of aspirin for primary prevention showed <strong>wildly inconsistent results</strong>. Some trials showed 15% reduction in stroke. Others showed 10% increase. The I¬≤ exceeded 60%. The explanation emerged slowly: aspirin prevents <strong>ischemic</strong> stroke but causes <strong>hemorrhagic</strong> stroke. When trials measured 'all stroke,' results depended on the baseline mix of stroke types in each population. The inconsistency wasn't noise‚Äîit was biology. Understanding <em>why</em> studies differ changes interpretation entirely."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üîç", title: "Scenario: The Explainable Inconsistency",
        xp: 25,
        scenario: "A meta-analysis shows I¬≤ = 70%. However, subgroup analysis reveals: trials in high-risk patients show consistent benefit (I¬≤ = 15%, OR 0.60), while trials in low-risk patients show consistent null effects (I¬≤ = 20%, OR 1.02). The high I¬≤ is entirely explained by mixing these populations.",
        question: "Should you downgrade for inconsistency?",
        options: [
          { letter: "A", text: "Yes‚ÄîI¬≤ = 70% always requires downgrading", correct: false },
          { letter: "B", text: "No‚Äîinconsistency is fully explained by a pre-specified, biologically plausible subgroup", correct: true },
          { letter: "C", text: "Partially‚Äîdowngrade one level despite the explanation", correct: false },
          { letter: "D", text: "Report two separate certainty ratings for each subgroup", correct: false }
        ],
        feedback: {
          correct: "Correct! GRADE distinguishes unexplained heterogeneity (downgrade) from heterogeneity fully explained by credible subgroup effects (no downgrade for inconsistency). The explanation must be pre-specified and biologically plausible‚Äînot post-hoc data dredging.",
          incorrect: "When heterogeneity is fully explained by a pre-specified, credible subgroup effect, the 'inconsistency' actually tells us something useful: treatment works differently in different populations. This informs clinical practice rather than undermining certainty."
        }
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Albumin Paradox", title: "28 Trials, Zero Answers",
        text: "Should critically ill patients receive albumin for fluid resuscitation? By 1998, the Cochrane Injuries Group had pooled <strong>28 trials</strong>. Their conclusion: albumin <strong>increases mortality by 68%</strong> (RR 1.68). Headlines screamed. NHS considered banning albumin. The evidence seemed damning.",
        followup: "But look closer at those 28 trials. I¬≤ was <strong>0%</strong>‚Äîno heterogeneity. A different problem lurked: the trials were tiny (many with <50 patients), measured different populations (burns, trauma, surgery), and many were decades old. The <strong>SAFE trial</strong> (6,997 patients, 2004) finally answered the question: albumin was <strong>equivalent to saline</strong>‚Äîno benefit, no harm. The pooled estimate from 28 small, disparate trials had been an artifact of random variation in underpowered studies, not a real signal. Consistency (low I¬≤) doesn't guarantee truth."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "‚öñÔ∏è", title: "Scenario: The Consistent but Concerning Evidence",
        xp: 25,
        scenario: "A meta-analysis pools 12 small trials (total n=400) with I¬≤ = 5%‚Äîvery low heterogeneity. The pooled effect is OR 0.50 (dramatic benefit), statistically significant. A colleague argues this deserves HIGH certainty because results are 'beautifully consistent.'",
        question: "What concern should you raise?",
        options: [
          { letter: "A", text: "None‚Äîlow I¬≤ means high certainty for inconsistency", correct: false },
          { letter: "B", text: "Low I¬≤ in small trials may reflect consistent bias, not consistent truth", correct: true },
          { letter: "C", text: "I¬≤ = 5% is too low and suggests publication bias", correct: false },
          { letter: "D", text: "The pooled estimate is too dramatic to be real", correct: false }
        ],
        feedback: {
          correct: "Correct! Small trials with consistent dramatic results should trigger suspicion. They may share the same biases (publication bias, similar flawed methods). The albumin meta-analysis had I¬≤ near 0% from 28 trials‚Äîbut the large SAFE trial showed null effect. Consistency in small trials ‚â† truth.",
          incorrect: "Low heterogeneity among small, potentially biased trials can be misleading. If all trials share similar biases (publication bias, poor methods), they'll show consistently biased results. The albumin case proves this: I¬≤ was low, but the effect was an artifact."
        }
      }},
      { type: "stats", theme: "dark", content: {
        title: "Understanding I¬≤ Thresholds",
        stats: [
          { value: "0-25%", label: "Low heterogeneity" },
          { value: "25-50%", label: "Moderate" },
          { value: "50-75%", label: "Substantial" },
          { value: ">75%", label: "Considerable" }
        ],
        caption: "But I¬≤ alone doesn't determine downgrading‚Äîthe prediction interval and clinical meaning matter more."
      }},
      { type: "checklist", theme: "dark", content: {
        title: "Inconsistency Assessment Checklist",
        items: [
          "Calculate I¬≤ and tau¬≤ ‚Äî but don't stop there",
          "Examine prediction interval ‚Äî does it cross clinically important thresholds?",
          "Look for overlapping confidence intervals between studies",
          "Check if heterogeneity can be explained by credible subgroups",
          "Consider if small trials are driving inconsistency (vs. large trial disagreement)"
        ]
      }},
      { type: "quiz", theme: "dark", content: {
        question: "Why is the prediction interval more important than I¬≤ for GRADE inconsistency assessment?",
        options: [
          { text: "Prediction intervals are easier to calculate", correct: false },
          { text: "Prediction intervals tell us what effect a future study might show", correct: true },
          { text: "I¬≤ is only valid for large meta-analyses", correct: false },
          { text: "Prediction intervals account for publication bias", correct: false }
        ],
        xp: 20,
        feedback: {
          correct: "Exactly. The prediction interval says: 'The next study from this distribution of effects will probably fall in this range.' If that range includes harm while the pooled estimate shows benefit, we're deeply uncertain about what will happen to the next patient.",
          incorrect: "The prediction interval captures uncertainty about what future studies (or the next patient) might experience. I¬≤ tells us whether heterogeneity exists; the prediction interval tells us whether it matters clinically."
        }
      }}
    ]
  },
  {
    id: 5, title: "Indirectness", subtitle: "Wrong question, wrong answer", xp: 170,
    slides: [
      { type: "title", theme: "purple-bg", content: { title: "Indirectness", subtitle: "The third domain: Does this evidence answer our actual question?" }},
      { type: "objectives", theme: "dark", content: {
        objectives: [
          "Recognize four types of indirectness: population, intervention, comparator, outcome",
          "Understand why surrogate outcomes mislead",
          "Apply indirectness assessment to real scenarios",
          "Decide when evidence is too indirect to trust"
        ],
        tip: "The best evidence for the wrong question is still the wrong answer."
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The CAST Catastrophe", title: "50,000 Dead from a Logical Treatment",
        text: "After a heart attack, arrhythmias predict death. <strong>Encainide and flecainide</strong> suppressed arrhythmias on ECG beautifully‚Äî85% reduction in premature ventricular contractions. The FDA approved them. Cardiologists prescribed them to hundreds of thousands of post-MI patients. The logic was irresistible: dangerous arrhythmias ‚Üí sudden death; suppress arrhythmias ‚Üí prevent death.",
        followup: "The CAST trial was designed to confirm what everyone already 'knew.' Instead, it delivered a verdict that stunned cardiology. Patients on anti-arrhythmic drugs died at <strong>3.6 times the rate</strong> of placebo. The trials were stopped early for harm. An estimated <strong>50,000 Americans died</strong> from drugs that perfectly fixed the surrogate (ECG) while causing the outcome (death). The evidence for arrhythmia suppression was HIGH certainty‚Äîbut it was indirect evidence for mortality. That indirectness killed 50,000 people."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìà", title: "Scenario: The Perfect Surrogate",
        xp: 25,
        scenario: "A new diabetes drug shows dramatic HbA1c reduction (1.5% improvement vs. placebo). The company argues that since HbA1c is a 'validated surrogate' for cardiovascular outcomes, certainty of CV benefit should be rated MODERATE. No CV outcome trials exist yet.",
        question: "How should you rate certainty for cardiovascular benefit?",
        options: [
          { letter: "A", text: "MODERATE‚ÄîHbA1c is a validated surrogate", correct: false },
          { letter: "B", text: "HIGH‚Äî1.5% reduction is clinically meaningful", correct: false },
          { letter: "C", text: "LOW or VERY LOW‚Äîdowngrade for indirectness (surrogate outcome)", correct: true },
          { letter: "D", text: "Cannot rate until CV trials are conducted", correct: false }
        ],
        feedback: {
          correct: "Correct! ACCORD showed intensive HbA1c lowering <em>increased</em> deaths. Rosiglitazone lowered HbA1c while increasing heart attacks. Surrogate outcomes require downgrading for indirectness, no matter how 'validated' they seem. Patient-important outcomes are required for high certainty.",
          incorrect: "CAST drugs fixed ECGs while tripling deaths. ACCORD lowered HbA1c while increasing mortality. 'Validated surrogates' have repeatedly failed. GRADE requires downgrading one or two levels when only surrogate outcomes are measured."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The Bone Density Betrayal", title: "When Stronger Bones Broke More",
        text: "Sodium fluoride increased bone mineral density dramatically‚Äî<strong>8% improvement per year</strong>. For osteoporosis, this seemed like a miracle. Bone density was the accepted surrogate for fracture risk. Higher density = fewer fractures. The math seemed inescapable.",
        followup: "Then came the trials measuring actual fractures. Sodium fluoride <strong>increased</strong> fracture rates despite improving bone density. The explanation: fluoride created dense but brittle bone‚Äîlike adding quantity without quality. The surrogate (density) moved in the right direction while the patient-important outcome (fractures) moved in the wrong direction. This case became a landmark in surrogate outcome skepticism."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "ü¶¥", title: "Scenario: The Bone Drug Decision",
        xp: 25,
        scenario: "A new osteoporosis drug shows 4% improvement in bone mineral density at 2 years. The company presents evidence of reduced fractures, but the fracture trials used a different formulation and dose than the one being approved. The FDA asks you to rate certainty of fracture benefit for the approved formulation.",
        question: "What indirectness concern applies?",
        options: [
          { letter: "A", text: "No concern‚Äîbone density is a validated surrogate", correct: false },
          { letter: "B", text: "Indirect for intervention‚Äîdifferent formulation/dose was tested", correct: true },
          { letter: "C", text: "Indirect for population‚Äîfracture trials may have different patients", correct: false },
          { letter: "D", text: "Indirect for outcome‚Äîfractures are not patient-important", correct: false }
        ],
        feedback: {
          correct: "Correct! Indirectness can apply to intervention, population, comparator, or outcome. Here, the formulation/dose tested for fractures differs from the one being approved. We cannot assume equivalence. This requires downgrading for indirect intervention.",
          incorrect: "The fracture evidence comes from a different formulation and dose. We're extrapolating from Drug A at dose X to Drug A at dose Y. That's intervention indirectness‚Äîthe evidence doesn't directly test what we're approving."
        }
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Rosiglitazone Reckoning", title: "FDA's 2.5 Million Patient Gamble",
        text: "In 2007, cardiologist Steven Nissen published a meta-analysis showing rosiglitazone (Avandia) increased heart attacks by <strong>43%</strong>. The drug had been prescribed to millions based on glucose control. But no large outcome trial had ever been required.",
        followup: "The FDA's logic had been: diabetes causes heart disease ‚Üí better glucose control should prevent it. That's <strong>indirectness</strong>‚Äîassuming a chain of causation without testing the final link. The FDA estimated rosiglitazone caused <strong>83,000 excess heart attacks</strong> before restrictions were imposed. The European Medicines Agency suspended it entirely. A drug approved on indirect evidence had harmed more patients than it helped."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üíä", title: "Scenario: The Head-to-Head Gap",
        xp: 25,
        scenario: "You need to compare Drug A vs. Drug B for your guideline. No head-to-head trials exist. You have: Drug A vs. placebo (RR 0.70) and Drug B vs. placebo (RR 0.65) from separate trials in different populations conducted 10 years apart.",
        question: "What type of indirectness applies?",
        options: [
          { letter: "A", text: "No indirectness‚Äîwe can compare the two placebo-controlled estimates", correct: false },
          { letter: "B", text: "Indirect comparisons require downgrading‚Äîdifferent trials break randomization", correct: true },
          { letter: "C", text: "Only indirect for population, not for comparison", correct: false },
          { letter: "D", text: "Network meta-analysis solves this indirectness", correct: false }
        ],
        feedback: {
          correct: "Correct! Comparing across trials breaks randomization. Drug A's trial may have sicker patients, different background care, different outcome definitions. The RR difference (0.70 vs. 0.65) may reflect trial differences, not drug differences. GRADE calls this 'indirect comparison' and requires downgrading.",
          incorrect: "Indirect comparisons are fundamentally different from head-to-head trials. When we compare Drug A vs. placebo (trial 1) to Drug B vs. placebo (trial 2), any difference between trials becomes a confounder. Network meta-analysis helps but doesn't eliminate this indirectness."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The Oseltamivir Extrapolation", title: "From Lab to Clinic to Lie",
        text: "Tamiflu (oseltamivir) was approved based on trials showing it reduced <strong>symptom duration by 21 hours</strong> in otherwise healthy adults. When pandemic flu threatened, governments extrapolated: if it shortens symptoms, it must prevent complications. If it works in healthy adults, it must work in high-risk patients.",
        followup: "Neither extrapolation was supported. Trials in <strong>high-risk patients</strong> (the ones who actually need protection) showed no benefit. Trials measuring <strong>hospitalizations and complications</strong> (the outcomes that matter) were never conducted‚Äîor never published. The ¬£473 million UK stockpile was based on indirect evidence: wrong population (healthy, not high-risk), wrong outcome (symptoms, not complications). The Cochrane review downgraded certainty for both types of indirectness."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üè•", title: "Scenario: The Population Extrapolation",
        xp: 25,
        scenario: "A treatment shows benefit in trials of patients aged 18-65 without comorbidities. Your guideline targets elderly nursing home residents with multiple comorbidities. The treatment has biologically plausible reasons to work differently in the elderly (altered metabolism, drug interactions).",
        question: "Should you downgrade for indirectness?",
        options: [
          { letter: "A", text: "No‚Äîage alone doesn't require downgrading", correct: false },
          { letter: "B", text: "Yes‚Äîbiological reasons for different effects in target population require downgrading", correct: true },
          { letter: "C", text: "Only if the elderly show different baseline risk", correct: false },
          { letter: "D", text: "Use subgroup analysis from the original trials instead", correct: false }
        ],
        feedback: {
          correct: "Correct! When there are biological reasons to expect different effects in the target population, downgrade for indirectness. Tamiflu worked in healthy adults but the question was about sick elderly patients‚Äîa population with very different biology. Extrapolation across populations with different biology requires downgrading.",
          incorrect: "Population indirectness applies when the trial population differs meaningfully from the guideline population, especially when there are biological reasons to expect different effects. Elderly with comorbidities may metabolize drugs differently, have drug interactions, and have different risk-benefit profiles."
        }
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Torcetrapib Tragedy", title: "$800 Million Buried in 82 Days",
        text: "Pfizer's torcetrapib was the most expensive drug development in history‚Äî<strong>$800 million</strong> over 15 years. It dramatically raised HDL cholesterol (the 'good' cholesterol) by 72%. The logic was irresistible: low HDL causes heart disease ‚Üí raising HDL should prevent it. Phase 2 trials showed perfect surrogate outcomes. Pfizer began building manufacturing plants before Phase 3 completed.",
        followup: "The ILLUMINATE trial enrolled <strong>15,067 patients</strong>. After just 82 days, the Data Safety Monitoring Board stopped the trial. Torcetrapib increased death by <strong>58%</strong> and cardiovascular events by 25%. The drug that perfectly improved HDL was killing patients. Pfizer's stock dropped $21 billion in one day. The entire HDL-raising hypothesis‚Äîbuilt on decades of surrogate evidence‚Äîcollapsed. No HDL-raising drug has ever prevented cardiovascular events. Surrogate perfection masked clinical catastrophe."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üíî", title: "Scenario: The Perfect Surrogate",
        xp: 25,
        scenario: "A new drug raises HDL cholesterol by 80%‚Äîmore than any previous agent. The company argues: 'We've proven HDL causally protects against heart disease. This drug will save millions of lives.' They request accelerated approval based on HDL improvement alone.",
        question: "Based on the torcetrapib experience, what certainty applies to cardiovascular benefit?",
        options: [
          { letter: "A", text: "HIGH‚ÄîHDL is a validated causal risk factor", correct: false },
          { letter: "B", text: "MODERATE‚Äîstrong mechanistic support warrants some confidence", correct: false },
          { letter: "C", text: "LOW to VERY LOW‚ÄîHDL-raising has consistently failed to translate to outcomes", correct: true },
          { letter: "D", text: "Cannot rate until mechanism is fully understood", correct: false }
        ],
        feedback: {
          correct: "Correct! Torcetrapib, dalcetrapib, evacetrapib‚Äîevery HDL-raising drug has failed to prevent cardiovascular events despite dramatic surrogate improvement. The entire drug class teaches us: manipulating a surrogate doesn't guarantee clinical benefit. Evidence from this class requires serious downgrading for indirectness.",
          incorrect: "Multiple HDL-raising drugs have shown dramatic surrogate improvement with no cardiovascular benefit (or actual harm). This drug class has definitively proven that HDL manipulation doesn't translate to outcomes. The surrogate relationship doesn't hold when pharmacologically manipulated."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The Erythropoietin Reversal", title: "When Better Blood Was Worse",
        text: "Cancer patients with anemia feel terrible‚Äîfatigue, weakness, shortness of breath. Erythropoietin-stimulating agents (ESAs) raised hemoglobin and made patients feel better. Quality of life improved. The FDA approved ESAs for cancer-related anemia. Oncologists prescribed them widely.",
        followup: "Then meta-analyses examining <strong>mortality</strong> emerged. Despite improving hemoglobin (surrogate) and quality of life, ESAs increased <strong>death by 10%</strong> and thromboembolic events by 57%. Patients felt better while dying faster. The FDA added a black box warning. ESA use plummeted. The lesson: even quality-of-life improvements can coexist with mortality harm. Surrogates that patients can feel aren't necessarily safer surrogates."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "ü©∏", title: "Scenario: The Quality of Life Surrogate",
        xp: 25,
        scenario: "A treatment dramatically improves a symptom surrogate‚Äîpatients feel much better, energy improves, daily function increases. Quality of life scores improve by 40%. No mortality data exists. The company argues this is 'patient-centered evidence' that should receive high certainty.",
        question: "What certainty applies to net clinical benefit?",
        options: [
          { letter: "A", text: "HIGH‚Äîpatient-reported outcomes matter most", correct: false },
          { letter: "B", text: "MODERATE‚Äîquality of life is a valid outcome", correct: false },
          { letter: "C", text: "LOW‚Äîimproved symptoms can coexist with mortality harm; need outcome data", correct: true },
          { letter: "D", text: "Depends on the specific condition", correct: false }
        ],
        feedback: {
          correct: "Correct! ESAs improved quality of life while increasing death by 10%. Feeling better is not the same as being better. Symptom improvement can mask harm to survival. GRADE requires downgrading for indirectness when mortality/serious harm data is absent, even with positive symptom outcomes.",
          incorrect: "The ESA experience proves that patients can feel dramatically better while dying faster. Quality of life improvement is real but doesn't guarantee net benefit. Without mortality and serious adverse event data, certainty must remain low regardless of symptom improvement."
        }
      }},
      { type: "checklist", theme: "dark", content: {
        title: "Four Types of Indirectness",
        items: [
          "Population ‚Äî Trial patients differ from guideline patients",
          "Intervention ‚Äî Trial intervention differs from recommendation",
          "Comparator ‚Äî Trial comparison differs from clinical question",
          "Outcome ‚Äî Trial measures surrogates instead of patient-important outcomes"
        ]
      }},
      { type: "quiz", theme: "dark", content: {
        question: "Why did the CAST anti-arrhythmic drugs kill 50,000 people despite perfect evidence for arrhythmia suppression?",
        options: [
          { text: "The trials were fraudulent", correct: false },
          { text: "The surrogate (arrhythmia suppression) didn't predict the outcome (death)", correct: true },
          { text: "Doctors used the drugs incorrectly", correct: false },
          { text: "The patient population was different from trials", correct: false }
        ],
        xp: 20,
        feedback: {
          correct: "Exactly. The evidence for arrhythmia suppression was impeccable. The drugs did exactly what they were supposed to do on ECG. But suppressing arrhythmias didn't prevent death‚Äîit caused it through different mechanisms. This is indirectness for outcome: measuring surrogates instead of patient-important endpoints.",
          incorrect: "The trials were well-conducted. The drugs worked perfectly for their intended purpose (suppressing arrhythmias). The problem was indirectness: arrhythmia suppression (the measured outcome) didn't predict mortality (the outcome that matters). Surrogate outcomes can mislead even when evidence is methodologically perfect."
        }
      }}
    ]
  },
  {
    id: 6, title: "Imprecision", subtitle: "Not enough data", xp: 150,
    slides: [
      { type: "title", theme: "dark", content: { title: "Imprecision", subtitle: "The fourth domain: Do we have enough data?" }},
      { type: "objectives", theme: "dark", content: {
        objectives: [
          "Understand why wide confidence intervals undermine certainty",
          "Apply the 'Optimal Information Size' concept",
          "Recognize when 95% CIs cross clinical decision thresholds",
          "Decide when to downgrade for imprecision"
        ],
        tip: "Statistical significance is not enough‚Äîconfidence intervals must be clinically narrow."
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The PROWESS Mirage", title: "When 1,690 Deaths Weren't Enough",
        text: "In 2001, Eli Lilly's drotrecogin alfa (Xigris) was approved for severe sepsis based on the PROWESS trial. The trial showed <strong>6.1% absolute mortality reduction</strong>‚Äîfrom 30.8% to 24.7%. The FDA approved it despite concerns. Cost: $6,800 per treatment. Lilly projected billions in revenue.",
        followup: "Then came the confidence interval analysis. The 95% CI for mortality reduction ranged from <strong>1.9% to 10.4%</strong>. The benefit might be huge (10.4%) or clinically marginal (1.9%). With a drug this expensive and risky (increased bleeding), that uncertainty mattered. Subsequent trials failed to replicate PROWESS. The ADDRESS trial in less severe sepsis showed <strong>no benefit</strong>. The PROWESS-SHOCK trial confirmed <strong>no mortality benefit</strong> even in severe sepsis. In 2011, Lilly withdrew Xigris worldwide. A decade of use, billions spent, and the imprecision had warned us from the start."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìê", title: "Scenario: The Wide Confidence Interval",
        xp: 25,
        scenario: "A trial shows a new treatment reduces mortality from 20% to 14%‚Äîa 6% absolute reduction (RR 0.70, p=0.04). However, the 95% CI for absolute risk reduction is 0.5% to 11.5%. The lower bound (0.5%) is below your minimum clinically important difference of 2%.",
        question: "Should you downgrade for imprecision?",
        options: [
          { letter: "A", text: "No‚Äîthe result is statistically significant", correct: false },
          { letter: "B", text: "No‚Äîthe point estimate shows clinically meaningful benefit", correct: false },
          { letter: "C", text: "Yes‚Äîthe CI includes clinically unimportant effects", correct: true },
          { letter: "D", text: "Yes‚Äîthe CI includes harm", correct: false }
        ],
        feedback: {
          correct: "Correct! Statistical significance doesn't guarantee clinical precision. If the true effect could be as small as 0.5% (below your minimum important difference), you can't confidently recommend the treatment. GRADE requires downgrading when CIs cross clinical decision thresholds, not just the null.",
          incorrect: "The CI's lower bound (0.5%) falls below the minimum clinically important difference (2%). This means the true effect might be too small to justify treatment costs/risks. Statistical significance isn't enough‚ÄîCIs must exclude clinically unimportant effects for high certainty."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The Optimal Information Size", title: "Why Sample Size Matters for Certainty",
        text: "Consider a simple thought experiment: You flip a coin 4 times and get 3 heads (75%). Would you bet your house that the coin is biased? Now flip 1,000 times and get 750 heads. Same proportion‚Äîbut utterly different certainty.",
        followup: "GRADE uses the <strong>Optimal Information Size (OIS)</strong> concept. This asks: 'How many events would a single adequately-powered trial need to reliably detect this effect?' If your meta-analysis has fewer events than the OIS, you're in the position of the person who flipped 4 coins‚Äîthe proportion might be right, but you don't have enough data to be confident. For mortality outcomes, OIS is typically <strong>200-400 events</strong>. Many celebrated meta-analyses have far fewer."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üî¢", title: "Scenario: The Small Meta-Analysis",
        xp: 25,
        scenario: "A meta-analysis of 8 RCTs (total n=1,200) shows a treatment prevents death with RR 0.65 (95% CI 0.45-0.95). The total number of deaths across all trials is 87. The pooled estimate is statistically significant. Your OIS calculation suggests 300 events would be needed.",
        question: "Should you downgrade for imprecision?",
        options: [
          { letter: "A", text: "No‚Äîstatistical significance means adequate precision", correct: false },
          { letter: "B", text: "No‚Äî8 RCTs provide adequate power through meta-analysis", correct: false },
          { letter: "C", text: "Yes‚Äî87 events is well below the OIS of 300", correct: true },
          { letter: "D", text: "Only if the CI crosses 1.0", correct: false }
        ],
        feedback: {
          correct: "Correct! 87 events vs. OIS of 300 means we have less than 30% of the information needed. The estimate might be right, but we're in 'small sample' territory where random variation can produce dramatic-looking but unreliable effects. GRADE requires downgrading when events are far below OIS.",
          incorrect: "Meta-analysis doesn't magically create information. 8 small trials with 87 total events don't equal one large trial with 300 events. The statistical significance is likely fragile‚Äîa few events either way would change the conclusion. OIS provides a reality check on sample adequacy."
        }
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Hydroxyethyl Starch Reversal", title: "When 10,000 Patients Weren't Enough... Then Were",
        text: "Hydroxyethyl starch (HES) fluids were used for decades in critically ill patients. Early meta-analyses with few events showed possible benefit. Then came the large trials: <strong>6S trial (798 patients)</strong>, <strong>CHEST trial (7,000 patients)</strong>, <strong>CRISTAL trial (2,857 patients)</strong>.",
        followup: "The large trials showed HES <strong>increased mortality and kidney failure</strong>. The earlier 'positive' evidence had been imprecise‚Äîconfidence intervals that allowed for benefit also allowed for the substantial harm that large trials eventually detected. The European Medicines Agency suspended HES for critical illness. Imprecise evidence had allowed a harmful intervention to persist for years."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "‚ö†Ô∏è", title: "Scenario: The Borderline Mortality Signal",
        xp: 25,
        scenario: "A meta-analysis shows RR for death of 1.25 (95% CI 0.98-1.60) with a treatment. The result 'trends toward harm' but isn't statistically significant. Your guideline must decide whether to recommend this treatment for a non-fatal condition where alternatives exist.",
        question: "How should imprecision affect your recommendation?",
        options: [
          { letter: "A", text: "The treatment is safe‚ÄîCI includes 1.0", correct: false },
          { letter: "B", text: "The treatment is harmful‚Äîpoint estimate shows 25% increase", correct: false },
          { letter: "C", text: "Serious uncertainty about harm‚ÄîCI consistent with up to 60% mortality increase", correct: true },
          { letter: "D", text: "Need more trials before any conclusion", correct: false }
        ],
        feedback: {
          correct: "Correct! The upper bound (1.60) represents a 60% mortality increase. For a non-fatal condition with alternatives, that possibility should drive decision-making. 'Not statistically significant' doesn't mean safe‚Äîit means we can't rule out substantial harm. GRADE incorporates the entire CI, not just p-values.",
          incorrect: "The CI's upper bound of 1.60 means we can't rule out 60% increased mortality. For a treatment addressing a non-fatal condition where alternatives exist, this uncertainty about serious harm should dominate the decision. Statistical non-significance ‚â† safety."
        }
      }},
      { type: "concept", theme: "dark", content: {
        title: "The Fragility Index",
        subtitle: "How stable is statistical significance?",
        text: "The fragility index asks: 'How many patients would need to have had a different outcome to make this result non-significant?' A fragility index of 3 means if just 3 patients had different outcomes, the p-value would cross 0.05. Many published 'significant' findings have fragility indices under 10.",
        highlight: "A trial with fragility index of 5 means the 'significant' result hangs on fewer patients than sit in your office waiting room."
      }},
      { type: "story", theme: "dark", content: {
        label: "The Tranexamic Acid Triumph", title: "When Precision Demanded 20,000 Patients",
        text: "Could tranexamic acid (TXA) reduce death from bleeding in trauma? Early trials were suggestive but imprecise. The <strong>CRASH-2 trial</strong> took a different approach: enroll enough patients to get a precise answer.",
        followup: "<strong>20,211 patients</strong> across 274 hospitals in 40 countries. Result: TXA reduced death from bleeding from 5.7% to 4.9%‚Äîa 0.8% absolute reduction. The 95% CI was <strong>0.3% to 1.3%</strong>. Finally, precision: the entire CI showed clinically meaningful benefit. The WHO added TXA to its Essential Medicines List. The precision wasn't just scientific‚Äîit changed global policy because decision-makers could trust the narrow bounds."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "‚úì", title: "Scenario: The Precise But Small Effect",
        xp: 25,
        scenario: "A mega-trial (n=40,000) shows a treatment reduces mortality with RR 0.95 (95% CI 0.91-0.99). The result is statistically significant and the CI is narrow. However, the absolute risk reduction is 0.3% (NNT = 333). Your minimum clinically important difference for this outcome is 0.2%.",
        question: "Should you downgrade for imprecision?",
        options: [
          { letter: "A", text: "Yes‚Äîthe effect is too small to matter clinically", correct: false },
          { letter: "B", text: "No‚Äîthe narrow CI excludes clinically unimportant effects", correct: true },
          { letter: "C", text: "Yes‚ÄîNNT of 333 suggests imprecision", correct: false },
          { letter: "D", text: "Cannot determine without knowing the OIS", correct: false }
        ],
        feedback: {
          correct: "Correct! The entire CI (0.91-0.99) lies below 1.0, and the corresponding absolute effects (0.1%-0.5%) are all above your minimum important difference of 0.2%. Small but precise effects that meet the clinical threshold don't require downgrading for imprecision. The issue is clinical judgment about whether the effect justifies intervention.",
          incorrect: "Imprecision is about confidence intervals, not effect size. A small but precisely estimated effect that excludes clinically unimportant values is not imprecise‚Äîit's just small. Whether a 0.3% absolute reduction justifies treatment is a values judgment, not a precision issue."
        }
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The JUPITER Controversy", title: "When 17,802 Patients Still Weren't Enough",
        text: "The JUPITER trial enrolled <strong>17,802 patients</strong> to test rosuvastatin for primary prevention. After just 1.9 years (planned: 5 years), it was stopped early for 'overwhelming benefit.' Heart attacks were reduced by 54%! Headlines celebrated. Rosuvastatin prescriptions soared.",
        followup: "But critics noticed problems. Only <strong>68 heart attacks</strong> total across both groups‚Äîfragility index of just 4. The wide confidence intervals (0.29-0.73) reflected genuine uncertainty. Early stopping inflates effect sizes by 30% on average. And absolute risk reduction was just <strong>0.35%</strong> over 1.9 years‚ÄîNNT of 286. A 'landmark' trial with thousands of patients still had imprecision concerns because it was stopped before enough events accumulated. Large n doesn't guarantee precision when event rates are low."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "‚èπÔ∏è", title: "Scenario: The Early-Stopped Trial",
        xp: 25,
        scenario: "A large trial (n=10,000) is stopped early after 2 years (planned: 5) for 'overwhelming efficacy.' The treatment shows 50% relative risk reduction. However, only 45 total events occurred, and the original planned stopping boundary was crossed by just 2 events.",
        question: "How should early stopping affect certainty assessment?",
        options: [
          { letter: "A", text: "No concern‚Äîlarge sample size ensures precision", correct: false },
          { letter: "B", text: "Downgrade for imprecision‚Äîearly stopping inflates effects and creates fragile findings", correct: true },
          { letter: "C", text: "Upgrade certainty‚Äîstopping early proves the effect is large", correct: false },
          { letter: "D", text: "Neutral‚Äîmonitoring rules are pre-specified", correct: false }
        ],
        feedback: {
          correct: "Correct! Early stopping typically inflates effect sizes by 30%. With only 45 events and crossing the boundary by 2, this finding is extremely fragile. Large n doesn't help when event rates are low‚Äîyou need enough events, not just patients. GRADE recommends downgrading for trials stopped early with few events.",
          incorrect: "Trials stopped early systematically overestimate effects. With 45 events and a fragility index of ~2, the 50% reduction would disappear if just 2 patients had different outcomes. The large sample is irrelevant‚Äîit's events that drive precision."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The ISIS-4 Lesson", title: "When 58,000 Patients Answered the Question",
        text: "After GISSI-2 suggested magnesium might reduce mortality in heart attack patients, smaller trials showed inconsistent results. The <strong>ISIS-4</strong> trial was designed to end the debate: <strong>58,050 patients</strong>, the largest heart attack trial ever. If magnesium worked, this trial would prove it.",
        followup: "Result: magnesium showed <strong>no benefit</strong> (mortality 7.64% vs. 7.24%, NS). The pooled meta-analysis before ISIS-4 had shown significant benefit‚Äîbut was driven by small trials with imprecise estimates. When finally tested with adequate precision, the effect vanished. This pattern‚Äîdramatic effects in small trials, null effects in large trials‚Äîis so common it has a name: 'the winner's curse.' Small trials select for extreme results; large trials reveal reality."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üé≤", title: "Scenario: Small Trial Meta-Analysis vs. Large Trial",
        xp: 25,
        scenario: "A meta-analysis of 15 small trials (total n=2,000, 180 events) shows significant mortality benefit (RR 0.65, p=0.001). A single large trial (n=15,000, 2,100 events) shows no benefit (RR 0.98, p=0.61). The meta-analysis was published first.",
        question: "How should you weight these conflicting findings?",
        options: [
          { letter: "A", text: "Average them‚Äîmore trials means more evidence", correct: false },
          { letter: "B", text: "Prefer the meta-analysis‚Äî15 trials beat 1 trial", correct: false },
          { letter: "C", text: "Prefer the large trial‚Äîit has 10x more events and precision", correct: true },
          { letter: "D", text: "Declare evidence inconclusive", correct: false }
        ],
        feedback: {
          correct: "Correct! 2,100 events vs. 180 events means the large trial has ~12x more information. Small trial meta-analyses are prone to publication bias and the winner's curse. When a large, adequately powered trial contradicts small trial meta-analyses, the large trial usually reveals the truth.",
          incorrect: "ISIS-4 taught this lesson. Small trials showed magnesium benefit; the 58,000-patient trial showed nothing. Events determine precision, not number of trials. Publication bias and random variation create false signals in small trial meta-analyses that large trials correct."
        }
      }},
      { type: "checklist", theme: "dark", content: {
        title: "Imprecision Assessment Checklist",
        items: [
          "Does the 95% CI cross clinically important thresholds?",
          "Does the CI cross both benefit and harm?",
          "Is the total number of events below the Optimal Information Size?",
          "What is the fragility index for key findings?",
          "Would the upper or lower bound change clinical decision-making?"
        ]
      }},
      { type: "quiz", theme: "dark", content: {
        question: "Why does GRADE require downgrading when confidence intervals cross 'clinical decision thresholds' even if statistically significant?",
        options: [
          { text: "Statistical conventions are arbitrary", correct: false },
          { text: "The true effect might be too small (or too harmful) to justify treatment", correct: true },
          { text: "P-values don't account for sample size", correct: false },
          { text: "Confidence intervals are more accurate than p-values", correct: false }
        ],
        xp: 20,
        feedback: {
          correct: "Exactly. If a treatment 'significantly' reduces mortality but the CI includes effects too small to justify the cost/risk‚Äîor includes substantial harm‚Äîwe can't confidently recommend it. GRADE judges precision against clinical thresholds, not statistical conventions.",
          incorrect: "Clinical decisions require knowing whether the true effect is large enough to matter. A statistically significant result with a CI spanning 'trivially helpful' to 'substantially harmful' doesn't guide treatment. GRADE uses clinical thresholds because that's what decisions require."
        }
      }}
    ]
  },
  {
    id: 7, title: "Publication Bias", subtitle: "The invisible evidence", xp: 160,
    slides: [
      { type: "title", theme: "red-bg", content: { title: "Publication Bias", subtitle: "The fifth domain: What evidence never reached us?" }},
      { type: "objectives", theme: "dark", content: {
        objectives: [
          "Understand how negative trials disappear from the literature",
          "Interpret funnel plots and their limitations",
          "Recognize when publication bias likely distorts evidence",
          "Decide when to downgrade certainty for publication bias"
        ],
        tip: "The most dangerous evidence is evidence you never see."
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Paroxetine Deception", title: "GlaxoSmithKline's Hidden Children",
        text: "In the early 2000s, GSK marketed paroxetine (Paxil) for adolescent depression. The published evidence looked convincing. Study 329, published in the <em>Journal of the American Academy of Child and Adolescent Psychiatry</em>, reported paroxetine was <strong>'generally well tolerated and effective.'</strong> Thousands of teenagers were prescribed the drug.",
        followup: "Then lawsuits forced GSK to release internal documents. Study 329 had actually shown paroxetine was <strong>no better than placebo</strong> and increased suicidal behavior. GSK had conducted other trials‚Äîalso negative‚Äîthat were never published. The published paper had been ghost-written by a medical communications company. When all studies were combined, the truth emerged: paroxetine didn't work for teen depression and was dangerous. The published literature had been systematically manipulated. Publication bias wasn't passive‚Äîit was manufactured."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìö", title: "Scenario: The All-Positive Literature",
        xp: 25,
        scenario: "A meta-analysis includes 12 industry-funded trials of a new antidepressant‚Äîall showing benefit. The funnel plot appears symmetric. A colleague notes that FDA records show the company conducted 8 additional trials that were never published. You cannot access the unpublished data.",
        question: "Should you downgrade for publication bias?",
        options: [
          { letter: "A", text: "No‚Äîthe funnel plot is symmetric", correct: false },
          { letter: "B", text: "No‚Äîwe can only judge evidence that exists", correct: false },
          { letter: "C", text: "Yes‚Äîdocumented unpublished trials suggest selective reporting", correct: true },
          { letter: "D", text: "Request the data before deciding", correct: false }
        ],
        feedback: {
          correct: "Correct! The funnel plot only shows published studies‚Äîit can't detect missing studies. Knowing that 8 unpublished trials exist is strong evidence of publication bias. Paroxetine's symmetric funnel plot concealed hidden negative trials. GRADE downgrades when there's direct evidence of unpublished studies.",
          incorrect: "Funnel plots only show what's published. If negative trials are systematically suppressed (as with paroxetine), the funnel can appear perfect while hiding crucial evidence. FDA records showing unpublished trials are direct evidence of publication bias."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The Oseltamivir Blockade", title: "Five Years to See the Truth",
        text: "The Cochrane Collaboration spent <strong>five years</strong> fighting Roche for Tamiflu (oseltamivir) data. Published trials showed reduced complications. Governments stockpiled billions of dollars worth of the drug. But Cochrane noticed discrepancies: study reports didn't match clinical study reports.",
        followup: "When Roche finally released the data in 2013, the missing <strong>60% of patient data</strong> revealed the truth. Published trials had reported 'complications' without proper definitions. Unpublished data showed adverse events were underreported. The updated Cochrane review: Tamiflu reduced symptoms by less than a day and had <strong>no proven effect on hospitalization or complications</strong>. Five years. Billions of dollars. And the published literature had been a carefully curated selection."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìä", title: "Scenario: The Asymmetric Funnel",
        xp: 25,
        scenario: "A funnel plot shows clear asymmetry‚Äîsmall studies cluster toward positive effects while negative small studies are absent. Egger's test is statistically significant (p=0.01). The meta-analysis includes 25 trials with a pooled OR of 0.65 (significant benefit).",
        question: "How should this affect your certainty rating?",
        options: [
          { letter: "A", text: "No effect‚ÄîEgger's test has poor specificity", correct: false },
          { letter: "B", text: "Downgrade one level‚Äîasymmetry suggests missing negative small studies", correct: true },
          { letter: "C", text: "Exclude small studies and re-analyze", correct: false },
          { letter: "D", text: "Asymmetry could be small-study effects, not publication bias", correct: false }
        ],
        feedback: {
          correct: "Correct! Significant funnel asymmetry with missing negative small studies is classic publication bias. Small negative studies are less likely to be published than small positive ones. Egger's test p=0.01 supports genuine asymmetry. GRADE requires downgrading when funnel asymmetry is clearly present.",
          incorrect: "While asymmetry has multiple causes, missing negative small studies is the most common pattern. The scenario describes classic publication bias: small positive studies published, small negative studies missing. This requires downgrading even if the exact cause is uncertain."
        }
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Reboxetine Scandal", title: "74% of Data Hidden",
        text: "Reboxetine, an antidepressant approved in Europe, had <strong>74% of patient data</strong> from clinical trials never published. The published literature showed the drug worked. Regulatory agencies approved it. Doctors prescribed it to thousands of patients.",
        followup: "German researchers obtained the unpublished data through freedom of information requests. When all data was combined, reboxetine was <strong>no better than placebo</strong> and caused more side effects than other antidepressants. The published literature‚Äîbased on only 26% of the data‚Äîhad created a complete fiction. A drug that didn't work and caused harm had been sold for years because three-quarters of the evidence was invisible."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üóÉÔ∏è", title: "Scenario: The Registry Gap",
        xp: 25,
        scenario: "You're assessing trials of a cancer drug. ClinicalTrials.gov shows 15 trials were registered and completed between 2010-2018. Your systematic search finds only 9 published papers. The 6 unpublished trials were all smaller than the published ones.",
        question: "What should you conclude about publication bias?",
        options: [
          { letter: "A", text: "No bias‚Äîsmaller trials are often pilot studies", correct: false },
          { letter: "B", text: "Unclear‚Äîwe don't know the results of unpublished trials", correct: false },
          { letter: "C", text: "High likelihood of bias‚Äî40% of completed trials unpublished, all smaller ones", correct: true },
          { letter: "D", text: "Request unpublished results before rating", correct: false }
        ],
        feedback: {
          correct: "Correct! 40% unpublished rate‚Äîand specifically the smaller trials‚Äîis a red flag. Small trials showing positive results get published; small trials showing nothing often don't. This pattern inflates pooled estimates. GRADE recognizes registry-publication gaps as strong evidence of publication bias.",
          incorrect: "Six of 15 completed trials (40%) never published‚Äîand all were smaller studies. This is exactly the pattern that inflates meta-analyses: small negative trials disappear. We can't wait indefinitely for unpublished data‚Äîwe must acknowledge the bias exists."
        }
      }},
      { type: "stats", theme: "dark", content: {
        title: "The Scale of Hidden Evidence",
        stats: [
          { value: "50%", label: "Trials never published" },
          { value: "74%", label: "Reboxetine data hidden" },
          { value: "60%", label: "Tamiflu data unreleased" },
          { value: "31%", label: "Outcomes switched" }
        ],
        caption: "Published literature represents a curated selection, not complete evidence."
      }},
      { type: "concept", theme: "dark", content: {
        title: "Why Funnel Plots Aren't Enough",
        subtitle: "Detecting the undetectable",
        text: "Funnel plots can only show what's published. They cannot detect:<br>‚Ä¢ Entire drugs that failed in trials and were never developed further<br>‚Ä¢ Studies with outcome switching (positive outcomes published, negative ones hidden)<br>‚Ä¢ Industry studies kept confidential<br>‚Ä¢ Studies published in inaccessible journals",
        highlight: "When publication bias is well-executed, it leaves no visible trace in the published literature."
      }},
      { type: "checklist", theme: "dark", content: {
        title: "Publication Bias Red Flags",
        items: [
          "All or most trials industry-funded",
          "Trial registries show unpublished completed trials",
          "Funnel plot asymmetry with missing negative small studies",
          "Outcomes in publications differ from registered protocols",
          "Large effects in meta-analysis contradicted by single large trial"
        ]
      }},
      { type: "quiz", theme: "dark", content: {
        question: "Why did publication bias with paroxetine for adolescent depression go undetected for years?",
        options: [
          { text: "The studies were well-conducted", correct: false },
          { text: "Negative trials were hidden and the positive trial was ghost-written to misrepresent results", correct: true },
          { text: "The drug genuinely worked in some patients", correct: false },
          { text: "Regulatory agencies didn't review the data", correct: false }
        ],
        xp: 20,
        feedback: {
          correct: "Exactly. GSK hid negative trials and hired a medical communications company to write Study 329 in a way that made paroxetine look effective. The publication bias was deliberate and systematic‚Äîvisible only when lawsuits forced disclosure.",
          incorrect: "The deception was active, not passive. GSK knew paroxetine didn't work for teen depression and increased suicidal behavior. They buried negative trials and misrepresented the one published trial. Traditional detection methods couldn't find what was deliberately hidden."
        }
      }},
      { type: "story", theme: "red-bg", content: {
        label: "The Antidepressant Illusion", title: "Turner's 94 Trial Revelation",
        text: "In 2008, Dr. Erick Turner at the FDA did something unprecedented: he compared <strong>74 FDA-registered antidepressant trials</strong> to what was actually published. The FDA had seen all trials‚Äîpositive and negative. The literature had seen a carefully curated selection.",
        followup: "The results were damning. Of 38 positive trials, <strong>37 were published</strong>. Of 36 negative or questionable trials, only <strong>3 were published as negative</strong>‚Äî22 were never published, and 11 were published but 'spun' to appear positive. The published literature showed antidepressants were effective in 94% of trials. The FDA data showed 51%. Effect sizes in publications were <strong>32% larger</strong> than reality. An entire drug class had its efficacy inflated by systematic publication bias."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üíä", title: "Scenario: The Selective Literature",
        xp: 25,
        scenario: "You're assessing antidepressants for a guideline. The published literature shows 12 of 15 trials are positive. However, you discover FDA databases listing 8 additional completed trials that were never published. The published effect size is d=0.50.",
        question: "How should you adjust your certainty assessment?",
        options: [
          { letter: "A", text: "Use only published trials‚Äîunpublished data isn't peer-reviewed", correct: false },
          { letter: "B", text: "Downgrade significantly‚Äî8 unpublished trials likely changes the effect size substantially", correct: true },
          { letter: "C", text: "Assume unpublished trials showed similar results", correct: false },
          { letter: "D", text: "Request unpublished data before making any assessment", correct: false }
        ],
        feedback: {
          correct: "Correct! Turner's analysis showed published effect sizes were 32% inflated. Eight unpublished trials out of 23 total (35% hidden) means your effect estimate is likely substantially biased. Downgrade for publication bias and note that the true effect is probably smaller than published estimates.",
          incorrect: "The antidepressant literature showed 94% positive trials in publications vs. 51% in FDA data. Unpublished trials are systematically negative. Assuming they're similar to published trials contradicts decades of evidence about publication bias patterns."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The RECORD Disaster", title: "When a Safety Study Hid Safety Signals",
        text: "After Avandia's cardiovascular concerns emerged, GSK designed RECORD‚Äîa 'cardiovascular safety study' of rosiglitazone. The study enrolled <strong>4,447 patients</strong> and ran for 5+ years. It was specifically designed to assess cardiovascular safety. The published conclusion: 'Rosiglitazone did not significantly increase cardiovascular risk.'",
        followup: "An FDA re-analysis told a different story. RECORD had serious adjudication problems‚Äî<strong>35% of potential cardiovascular events weren't properly evaluated</strong>. Some patients' events were classified as 'non-cardiac' when records suggested otherwise. The study designed to answer the safety question had been conducted in ways that systematically underdetected the very events it was supposed to measure. The publication bias wasn't in what studies were published‚Äîit was in how the study itself was designed and analyzed to produce a publishable 'negative' safety finding."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üîç", title: "Scenario: The 'Reassuring' Safety Study",
        xp: 25,
        scenario: "A drug with suspected harm has a 'definitive' safety study published showing no increased risk. However, you notice: outcomes were adjudicated by company employees, 30% of potential events were excluded as 'protocol deviations,' and the comparison group had higher baseline risk.",
        question: "How should these methodological features affect your interpretation?",
        options: [
          { letter: "A", text: "Accept the reassurance‚Äîthe study was specifically designed for safety", correct: false },
          { letter: "B", text: "Be skeptical‚Äîsystematic bias in outcome ascertainment can hide real signals", correct: true },
          { letter: "C", text: "Partially reassured‚Äîsome bias doesn't eliminate all value", correct: false },
          { letter: "D", text: "Ignore and wait for independent replication", correct: false }
        ],
        feedback: {
          correct: "Correct! RECORD taught us that 'safety studies' can be designed to not find harm. Company adjudication, selective exclusions, and favorable comparisons can systematically underdetect adverse events. A 'negative' safety study with these features provides false reassurance.",
          incorrect: "A safety study designed and adjudicated by the company with suspected product has obvious conflicts. RECORD's 35% non-evaluation rate meant events weren't being properly counted. 'Designed for safety' is meaningless if the design allows harm to escape detection."
        }
      }}
    ]
  },
  {
    id: 8, title: "Upgrading Factors", subtitle: "When observational evidence earns trust", xp: 140,
    slides: [
      { type: "title", theme: "green-bg", content: { title: "Upgrading Factors", subtitle: "When observation rivals randomization" }},
      { type: "objectives", theme: "dark", content: {
        objectives: [
          "Understand the three GRADE upgrading factors",
          "Learn when large effects can overcome confounding",
          "Recognize dose-response gradients as evidence of causation",
          "Apply 'all plausible confounders' reasoning"
        ],
        tip: "Upgrading is rare‚Äîbut sometimes observational evidence is unassailable."
      }},
      { type: "story", theme: "green-bg", content: {
        label: "The Smoking Proof", title: "How 30x Risk Silenced Doubt",
        text: "In 1950, Richard Doll and Austin Bradford Hill published their case-control study: smokers had <strong>30 times higher lung cancer rates</strong> than non-smokers. The tobacco industry demanded RCTs‚Äîknowing they were impossible. 'Correlation isn't causation,' they insisted. 'Where's the randomized evidence?'",
        followup: "But the effect size spoke for itself. For confounding to explain a 30-fold increase, some unmeasured factor would need to cause both smoking and lung cancer with extraordinary strength‚Äîand be distributed perfectly across populations, generations, and geographies. No such confounder has ever been found. The evidence was upgraded from LOW to HIGH. GRADE recognizes that some effects are too large for confounding to explain."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìà", title: "Scenario: The Large Effect",
        xp: 25,
        scenario: "An observational study shows a surgical procedure reduces mortality from 80% to 10%‚Äîa relative risk of 0.125 (87.5% relative reduction). No RCTs exist because withholding treatment would be unethical given the dramatic observed benefit.",
        question: "Should this evidence be upgraded for large effect?",
        options: [
          { letter: "A", text: "No‚Äîobservational evidence cannot be upgraded", correct: false },
          { letter: "B", text: "Yes‚ÄîRR 0.125 is so large that confounding alone is implausible", correct: true },
          { letter: "C", text: "Only if the study is well-designed", correct: false },
          { letter: "D", text: "Only if dose-response is also present", correct: false }
        ],
        feedback: {
          correct: "Correct! RR 0.125 (or 8-fold relative reduction) is in the range where confounding alone is implausible. No unmeasured confounder could realistically reduce mortality from 80% to 10%. GRADE upgrades for large effects when RR < 0.5 or > 2, with additional upgrading for very large effects (RR < 0.2 or > 5).",
          incorrect: "Effects this large are essentially RCT-proof. No confounder could plausibly explain an 87.5% relative reduction. Parachutes have never been tested in RCTs, but we don't doubt they work‚Äîthe effect size is self-evident. GRADE allows upgrading for effects too large for confounding to explain."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "The Folate Discovery", title: "Neural Tube Defects Prevented",
        text: "By the 1980s, observational studies suggested folate supplementation prevented neural tube defects (NTDs) like spina bifida. But how could anyone be certain? Confounding seemed inevitable‚Äîwomen who take vitamins are likely healthier in many ways.",
        followup: "Three upgrading factors came together. <strong>Large effect:</strong> Folate reduced NTDs by 70-80%. <strong>Dose-response:</strong> Higher folate levels correlated with lower NTD risk in a clear gradient. <strong>Confounding direction:</strong> Women who take supplements tend to be healthier‚Äîwhich would make folate look beneficial even if it weren't. But folate's effect persisted after adjusting for socioeconomic factors. When an effect survives adjustment for confounders that should bias toward it, the effect is likely real. The MRC RCT eventually confirmed 72% reduction‚Äîthe observational data had been right."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìâ", title: "Scenario: The Dose-Response Question",
        xp: 25,
        scenario: "A series of cohort studies shows alcohol consumption and liver cirrhosis. Results: 1-2 drinks/day (RR 1.2), 3-4 drinks/day (RR 2.1), 5-6 drinks/day (RR 4.5), 7+ drinks/day (RR 9.3). The gradient is consistent across studies and populations.",
        question: "Does this dose-response pattern support upgrading?",
        options: [
          { letter: "A", text: "No‚Äîall effects are from observational studies", correct: false },
          { letter: "B", text: "Yes‚Äîa consistent dose-response gradient across studies suggests causation", correct: true },
          { letter: "C", text: "Only if RR exceeds 2 at all dose levels", correct: false },
          { letter: "D", text: "Only if confounders show the same gradient", correct: false }
        ],
        feedback: {
          correct: "Correct! A clear dose-response gradient (1.2 ‚Üí 2.1 ‚Üí 4.5 ‚Üí 9.3) is hard to explain by confounding. A confounder would need to perfectly track alcohol intake at every level across diverse populations. This biological gradient supports causation and warrants upgrading.",
          incorrect: "Dose-response relationships provide strong evidence for causation. For confounding to create this pattern, some unknown factor would need to increase in perfect proportion to alcohol intake, across every study and population. That's implausible‚Äîthe gradient points to causation."
        }
      }},
      { type: "story", theme: "green-bg", content: {
        label: "The Confounding Paradox", title: "When Bias Proves the Case",
        text: "Consider the observation that motorcycle helmets reduce head injury deaths. Critics might argue: 'Maybe helmet-wearers are more cautious drivers.' That's confounding‚Äîand it's probably true. Cautious drivers likely do wear helmets more often.",
        followup: "But here's the paradox: if cautious driving confounds the relationship, it should bias <em>toward</em> helmets looking beneficial. Cautious drivers have fewer crashes and thus fewer head injuries. If helmets still show dramatic benefit <em>despite</em> this confounding working in their favor, the true effect might be even larger. When all plausible confounders bias in one direction and the effect persists (or is opposite), causation becomes more credible. This 'all plausible confounders' reasoning is GRADE's third upgrading criterion."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üîÑ", title: "Scenario: The Confounding Direction",
        xp: 25,
        scenario: "A registry study shows that a new surgical technique reduces complications (RR 0.60). You suspect confounding: healthier patients probably receive the new technique. Healthier patients would have fewer complications anyway. This confounding should bias the results toward showing benefit.",
        question: "Does this confounding pattern support or undermine causation?",
        options: [
          { letter: "A", text: "Undermines‚Äîconfounding invalidates observational studies", correct: false },
          { letter: "B", text: "Supports‚Äîif benefit persists despite confounding that favors treatment, true benefit may exist", correct: true },
          { letter: "C", text: "Neutral‚Äîwe cannot determine confounding direction", correct: false },
          { letter: "D", text: "Undermines‚Äîwe need propensity score matching", correct: false }
        ],
        feedback: {
          correct: "Correct! If healthier patients get the new technique AND the technique still shows benefit, the true benefit might be real (or even underestimated). When confounding should bias toward benefit but benefit still appears after adjustment, that's evidence for causation. GRADE upgrades when 'all plausible confounders' would bias in the observed direction.",
          incorrect: "Think about the direction: healthy patient confounding should make the new technique look good. If the technique still shows benefit after adjusting for health status, the true effect is supported. The confounding direction helps interpret the finding."
        }
      }},
      { type: "concept", theme: "dark", content: {
        title: "The Upgrading Criteria Summary",
        subtitle: "Three paths to higher certainty",
        checkpoints: [
          { day: "LARGE", event: "RR > 2 or < 0.5 (upgrade 1); RR > 5 or < 0.2 (upgrade 2)" },
          { day: "DOSE", event: "Clear gradient between exposure and outcome" },
          { day: "CONFOUND", event: "All plausible confounders would reduce observed effect" }
        ]
      }},
      { type: "story", theme: "dark", content: {
        label: "The Thyroid Storm", title: "When 96% Survival Speaks for Itself",
        text: "Before propylthiouracil (PTU), thyroid storm had <strong>nearly 100% mortality</strong>. With PTU, mortality dropped to around <strong>4%</strong>. No RCT was ever conducted‚Äîit would have required letting patients die in the control arm.",
        followup: "This is the paradigm case for large effect upgrading. A treatment that changes survival from ~0% to ~96% doesn't need an RCT. No confounder could explain turning certain death into reliable survival. The evidence quality was upgraded to HIGH despite being entirely observational. Some effects are simply too dramatic to doubt."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "‚ö†Ô∏è", title: "Scenario: The Moderate Effect",
        xp: 25,
        scenario: "A cohort study shows a lifestyle intervention reduces heart disease with RR 0.75 (25% relative reduction). The study is large, well-designed, with consistent results. A colleague argues this deserves upgrading for large effect.",
        question: "Should you upgrade?",
        options: [
          { letter: "A", text: "Yes‚Äî25% reduction is clinically important", correct: false },
          { letter: "B", text: "No‚ÄîRR 0.75 is well within the range confounding can produce", correct: true },
          { letter: "C", text: "Yes‚Äîlarge sample size warrants upgrading", correct: false },
          { letter: "D", text: "Only if dose-response is also present", correct: false }
        ],
        feedback: {
          correct: "Correct! RR 0.75 is exactly the range where confounding thrives. The HRT disaster showed RR 0.50 for heart protection‚Äîentirely due to confounding. Large effect upgrading requires RR > 2 or < 0.5 (some use even stricter thresholds). Clinical importance ‚â† causal certainty.",
          incorrect: "RR 0.75 is modest and easily explained by healthy user bias, unmeasured confounders, or selection effects. HRT's observational RR of 0.50 was entirely confounded. Only effects so large that confounding is implausible (RR > 2 or < 0.5) qualify for upgrading."
        }
      }},
      { type: "checklist", theme: "dark", content: {
        title: "Upgrading Criteria Checklist",
        items: [
          "Large effect: RR > 2 or < 0.5 (very large: > 5 or < 0.2)",
          "Dose-response: Clear gradient across exposure levels",
          "Confounding: All plausible confounders would reduce the effect",
          "Upgrading is rare‚Äîmost observational evidence remains LOW",
          "Multiple upgrading factors can be applied if clearly present"
        ]
      }},
      { type: "quiz", theme: "dark", content: {
        question: "Why did the observational evidence for smoking and lung cancer achieve HIGH certainty despite no RCTs?",
        options: [
          { text: "The studies were methodologically perfect", correct: false },
          { text: "Large effect (30x risk) + dose-response + no plausible confounders could explain it", correct: true },
          { text: "Biological mechanism was well-understood", correct: false },
          { text: "Multiple study designs agreed", correct: false }
        ],
        xp: 20,
        feedback: {
          correct: "Exactly. A 30-fold risk increase is beyond what confounding can explain. The clear dose-response (more cigarettes = more cancer) added evidence. And no unmeasured confounder has ever been found that could explain the effect. All three upgrading criteria were met.",
          incorrect: "The combination was powerful: an effect so large confounding couldn't explain it, a dose-response gradient supporting causation, and the direction of any conceivable confounding wouldn't diminish the finding. All three upgrading factors pushed the evidence to HIGH certainty."
        }
      }}
    ]
  },
  {
    id: 9, title: "Putting It Together", subtitle: "Real guideline decisions", xp: 200,
    slides: [
      { type: "title", theme: "dark", content: { title: "Putting It Together", subtitle: "Applying all five domains to real guideline decisions" }},
      { type: "objectives", theme: "dark", content: {
        objectives: [
          "Apply all GRADE domains to complex real-world evidence",
          "Practice constructing evidence summaries",
          "Make defensible certainty judgments",
          "Translate certainty to recommendation strength"
        ],
        tip: "This module presents complete cases‚Äîyou'll use everything you've learned."
      }},
      { type: "story", theme: "red-bg", content: {
        label: "Case 1: The Statin Controversy", title: "Statins for Primary Prevention",
        text: "In 2016, a Lancet review claimed statins for primary prevention were supported by <strong>'overwhelming evidence.'</strong> The meta-analysis showed 25% relative reduction in major vascular events. Industry-funded trials dominated. Patient advocates and independent researchers disagreed vehemently.",
        followup: "Let's assess this systematically. <strong>Risk of bias:</strong> Most trials industry-funded with concerns about selective reporting. <strong>Inconsistency:</strong> Effects varied by baseline risk‚Äîbenefit clear in high-risk, uncertain in low-risk. <strong>Indirectness:</strong> Many trials included patients with existing disease (secondary prevention). <strong>Imprecision:</strong> For low-risk primary prevention, confidence intervals crossed clinical thresholds. <strong>Publication bias:</strong> Industry sponsorship raised concerns about selective publication."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üíä", title: "Scenario: Rating the Statin Evidence",
        xp: 30,
        scenario: "For statins in LOW-RISK primary prevention (10-year CVD risk <10%), you have: industry-funded RCTs, some risk of bias concerns, heterogeneous results by risk level, outcomes measured may include secondary prevention patients, and modest absolute benefits (NNT ~200-500 to prevent one event over 5 years).",
        question: "What certainty rating is most appropriate for CV event reduction in this population?",
        options: [
          { letter: "A", text: "HIGH‚Äîthe meta-analysis is large and shows significant benefit", correct: false },
          { letter: "B", text: "MODERATE‚Äîdowngrade one level for indirectness (mixed populations)", correct: false },
          { letter: "C", text: "LOW‚Äîdowngrade for indirectness AND imprecision for this specific population", correct: true },
          { letter: "D", text: "VERY LOW‚Äîmultiple serious concerns across domains", correct: false }
        ],
        feedback: {
          correct: "Correct! For low-risk primary prevention specifically: indirectness (trials mixed with higher-risk patients) and imprecision (modest absolute benefits, NNT 200-500) both warrant downgrading. The certainty differs by population‚Äîwhat's HIGH for secondary prevention may be LOW for low-risk primary prevention.",
          incorrect: "The question asks specifically about LOW-RISK primary prevention. The overall meta-analysis includes higher-risk populations where benefit is clearer. For the specific population in the question, indirectness and imprecision both apply. GRADE is population-specific."
        }
      }},
      { type: "story", theme: "dark", content: {
        label: "Case 2: The Hydroxychloroquine Saga", title: "COVID-19 and Premature Certainty",
        text: "In March 2020, a French study of <strong>36 patients</strong> claimed hydroxychloroquine cleared COVID-19 viral load. The study was open-label, non-randomized, with selective exclusion of patients who died or went to ICU. Despite this, world leaders promoted it and millions of doses were prescribed.",
        followup: "Systematic GRADE assessment would have revealed: <strong>Risk of bias:</strong> CRITICAL (non-randomized, open-label, selective exclusion). <strong>Imprecision:</strong> SERIOUS (36 patients, 6 in treatment arm). <strong>Indirectness:</strong> SERIOUS (viral load, not clinical outcomes). Certainty: <strong>VERY LOW</strong>. Subsequent RCTs (RECOVERY, WHO Solidarity) showed no benefit. The pandemic revealed what happens when certainty assessment is bypassed."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "ü¶†", title: "Scenario: Assessing Pandemic Evidence",
        xp: 30,
        scenario: "A new respiratory virus emerges. A single-center study of 80 patients (non-randomized, open-label) shows a repurposed drug reduces ICU admission from 25% to 12%. The results are promoted as 'promising' and generate massive public demand.",
        question: "How should this evidence be rated and communicated?",
        options: [
          { letter: "A", text: "MODERATE‚Äîthe effect size is large and clinically meaningful", correct: false },
          { letter: "B", text: "LOW‚Äîobservational study with serious limitations", correct: false },
          { letter: "C", text: "VERY LOW‚Äîcritical risk of bias, serious imprecision; communicate uncertainty clearly", correct: true },
          { letter: "D", text: "Cannot rate‚Äîmore studies needed before any assessment", correct: false }
        ],
        feedback: {
          correct: "Correct! Non-randomized, open-label, single-center, n=80: VERY LOW certainty. The effect might be real, might be selection bias, might be confounding. Communicating this uncertainty‚Äînot dismissing the evidence, but properly contextualizing it‚Äîis essential for informed decision-making.",
          incorrect: "This scenario mirrors hydroxychloroquine. Critical risk of bias (non-randomized, open-label) plus serious imprecision (80 patients) yields VERY LOW certainty. That doesn't mean 'ignore it'‚Äîit means communicate the profound uncertainty while awaiting better evidence."
        }
      }},
      { type: "story", theme: "green-bg", content: {
        label: "Case 3: The Insulin Pump Victory", title: "When Certainty Builds Slowly",
        text: "Continuous subcutaneous insulin infusion (pump therapy) for type 1 diabetes accumulated evidence over decades. Early trials were small. Outcomes varied. No single trial was definitive. But over time, the mosaic came together.",
        followup: "By 2010, meta-analyses showed: consistent HbA1c improvement (~0.3%), reduced severe hypoglycemia, improved quality of life. <strong>Risk of bias:</strong> Some concerns but improving over time. <strong>Inconsistency:</strong> Low for HbA1c, moderate for hypoglycemia (likely explainable by patient selection). <strong>Indirectness:</strong> None‚Äîdirect outcomes in target population. <strong>Imprecision:</strong> Resolved with cumulative trials. <strong>Publication bias:</strong> Low concern with trials in multiple settings. Final rating: <strong>MODERATE to HIGH</strong> for HbA1c improvement, with appropriate nuance for other outcomes."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìã", title: "Scenario: The Evolving Evidence Base",
        xp: 25,
        scenario: "You're updating a guideline. Five years ago, certainty was LOW for a treatment. Since then: 3 new large RCTs with low risk of bias, consistent results (I¬≤=18%), direct population and outcomes, total events now exceed OIS, no evidence of publication bias.",
        question: "What certainty rating is now appropriate?",
        options: [
          { letter: "A", text: "Remains LOW‚Äîinitial rating shouldn't change dramatically", correct: false },
          { letter: "B", text: "MODERATE‚Äînew evidence improves but doesn't transform certainty", correct: false },
          { letter: "C", text: "HIGH‚Äînew RCTs with no serious concerns across domains", correct: true },
          { letter: "D", text: "Requires independent replication before upgrading", correct: false }
        ],
        feedback: {
          correct: "Correct! GRADE ratings should evolve as evidence evolves. Starting HIGH (RCTs), no downgrading needed: low risk of bias, consistent (I¬≤=18%), direct outcomes, adequate precision (exceeds OIS), no publication bias concern. The evidence now supports HIGH certainty.",
          incorrect: "GRADE is a living framework. As evidence accumulates and addresses prior concerns, certainty should be re-evaluated. Three large, low-bias RCTs with consistent results addressing all domains can absolutely raise certainty from LOW to HIGH."
        }
      }},
      { type: "story", theme: "red-bg", content: {
        label: "Case 4: The Arthroscopy Lesson Revisited", title: "When Guidelines Persist Despite Evidence",
        text: "By 2008, two sham-surgery RCTs had shown knee arthroscopy for osteoarthritis was no better than placebo. The evidence was HIGH certainty‚Äîwell-designed RCTs, consistent results, direct outcomes. Yet guidelines took years to change, and the procedure continued.",
        followup: "This reveals GRADE's limitation: certainty ratings inform decisions but don't make them. Surgeons argued: 'My patients improve.' Patients wanted the procedure. Systems had invested in it. A HIGH certainty of 'no benefit' should have ended the practice‚Äîbut implementation requires more than evidence rating. GRADE tells us what the evidence says; behavior change requires understanding why evidence gets ignored."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üî¨", title: "Scenario: The Evidence-Practice Gap",
        xp: 25,
        scenario: "Your guideline panel rates evidence as HIGH certainty that a common procedure is no better than placebo. The procedure is lucrative, widely performed, and patients report satisfaction. Panel members disagree about the recommendation strength.",
        question: "What recommendation strength is appropriate given HIGH certainty of no benefit?",
        options: [
          { letter: "A", text: "Strong recommendation AGAINST‚Äîhigh certainty of no benefit demands strong action", correct: true },
          { letter: "B", text: "Conditional recommendation against‚Äîpatient preferences matter", correct: false },
          { letter: "C", text: "No recommendation‚Äîpractice patterns should determine guidelines", correct: false },
          { letter: "D", text: "Conditional recommendation for‚Äîpatients report benefit", correct: false }
        ],
        feedback: {
          correct: "Correct! HIGH certainty that a procedure provides no benefit over placebo justifies a STRONG recommendation against. Patient-reported benefit in unblinded settings is placebo effect. Resource use for ineffective treatments has opportunity costs. High certainty + unfavorable balance = strong recommendation.",
          incorrect: "When certainty is HIGH and the balance is clearly unfavorable (no benefit, surgical risks, costs), GRADE supports a strong recommendation. Patient satisfaction with placebo doesn't justify continued use of invasive procedures. Strong evidence should drive strong recommendations."
        }
      }},
      { type: "concept", theme: "dark", content: {
        title: "From Certainty to Recommendations",
        subtitle: "Evidence quality is only part of the equation",
        text: "GRADE separates certainty of evidence from strength of recommendations. A recommendation considers:<br><br>‚Ä¢ Certainty of evidence<br>‚Ä¢ Balance of benefits vs. harms<br>‚Ä¢ Patient values and preferences<br>‚Ä¢ Resource use and feasibility",
        highlight: "Even LOW certainty evidence can support a STRONG recommendation (e.g., parachutes) if benefits clearly outweigh harms."
      }},
      { type: "story", theme: "dark", content: {
        label: "Case 5: The Transfusion Threshold", title: "When Less Is More",
        text: "For decades, blood transfusion thresholds were set at hemoglobin <10 g/dL‚Äîthe '10/30 rule.' The practice seemed logical: more blood should be better for anemic patients. Then came the RCTs.",
        followup: "The TRICC trial (1999) and subsequent studies showed <strong>restrictive transfusion (Hb <7 g/dL)</strong> was as safe or safer than liberal transfusion in most patients. The evidence accumulated: <strong>Risk of bias:</strong> Low in major trials. <strong>Inconsistency:</strong> Low across settings. <strong>Indirectness:</strong> Some subgroups less studied. <strong>Imprecision:</strong> Resolved with multiple trials. <strong>Publication bias:</strong> Unlikely given consistent direction. Certainty: <strong>MODERATE to HIGH</strong> for most populations. Guidelines changed. A practice that had seemed obviously beneficial was actually harmful."
      }},
      { type: "decision", theme: "dark", content: {
        icon: "ü©∏", title: "Scenario: The Restrictive Strategy",
        xp: 25,
        scenario: "Multiple RCTs show restrictive blood transfusion is as safe as liberal transfusion, with lower mortality in some subgroups. However, for acute coronary syndrome patients, only one small RCT exists with wide confidence intervals crossing clinically important thresholds.",
        question: "How should certainty differ between general hospital patients and ACS patients?",
        options: [
          { letter: "A", text: "Same certainty‚Äîthe overall evidence applies to all patients", correct: false },
          { letter: "B", text: "Lower for ACS‚Äîimprecision and indirectness require separate assessment", correct: true },
          { letter: "C", text: "Higher for ACS‚Äîthey have more to lose from transfusion complications", correct: false },
          { letter: "D", text: "Cannot rate ACS‚Äîinsufficient evidence for any conclusion", correct: false }
        ],
        feedback: {
          correct: "Correct! ACS patients have biological reasons for different responses (myocardial oxygen demand). One small imprecise RCT in this population means indirectness (extrapolating from general patients) or imprecision (direct ACS evidence) both warrant lower certainty. GRADE is population-specific.",
          incorrect: "GRADE requires assessing certainty for each specific population. ACS patients may respond differently than general hospital patients. One small imprecise trial directly in ACS patients doesn't provide the same certainty as the broader evidence base for other populations."
        }
      }},
      { type: "quiz", theme: "dark", content: {
        question: "What is the most important lesson from the hydroxychloroquine COVID-19 experience?",
        options: [
          { text: "Observational studies are always unreliable", correct: false },
          { text: "Proper GRADE assessment should precede clinical adoption and policy", correct: true },
          { text: "Repurposed drugs don't work for new diseases", correct: false },
          { text: "Media should not report on medical studies", correct: false }
        ],
        xp: 20,
        feedback: {
          correct: "Exactly. A 36-patient, non-randomized, open-label study with selective exclusions was VERY LOW certainty by any GRADE assessment. Systematic certainty evaluation before adoption would have prevented widespread use of an ineffective treatment and maintained public trust.",
          incorrect: "The lesson isn't that small studies are worthless‚Äîit's that they need proper certainty assessment. A rigorous GRADE evaluation would have labeled this VERY LOW certainty immediately. Systematic assessment must precede clinical adoption, especially during crises when pressure is highest."
        }
      }}
    ]
  },
  {
    id: 10, title: "Final Assessment", subtitle: "Test your mastery", xp: 250,
    slides: [
      { type: "title", theme: "dark", content: { title: "Final Assessment", subtitle: "Demonstrate your mastery of GRADE certainty assessment" }},
      { type: "concept", theme: "dark", content: {
        title: "The Final Challenge",
        subtitle: "Apply everything you've learned",
        text: "This module presents complex scenarios requiring integrated GRADE assessment. You'll need to consider multiple domains simultaneously and make nuanced judgments.",
        highlight: "Aim for 80% accuracy to earn the GRADE Master badge!"
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìä", title: "Challenge 1: The New Cancer Drug",
        xp: 35,
        scenario: "A new cancer drug shows median survival improvement from 11 to 14 months in a single industry-funded RCT (n=600). Primary outcome was changed mid-trial from overall survival to progression-free survival. Three of seven pre-specified secondary outcomes were not reported. The survival benefit was a post-hoc analysis.",
        question: "What certainty rating is appropriate for overall survival benefit?",
        options: [
          { letter: "A", text: "HIGH‚ÄîRCT showing survival benefit", correct: false },
          { letter: "B", text: "MODERATE‚Äîsingle trial but clinically meaningful", correct: false },
          { letter: "C", text: "LOW‚Äîdowngrade for risk of bias (outcome switching, selective reporting)", correct: true },
          { letter: "D", text: "VERY LOW‚Äîmultiple serious concerns", correct: false }
        ],
        feedback: {
          correct: "Correct! Key risk of bias concerns: (1) primary outcome changed mid-trial, (2) unreported secondary outcomes, (3) survival benefit was post-hoc. Each suggests selective reporting. Starting HIGH, downgrade one or two levels for serious risk of bias. Industry funding adds concern but isn't sufficient alone for downgrading.",
          incorrect: "The trial shows classic selective reporting: changing primary endpoint, hiding unfavorable secondary outcomes, promoting post-hoc survival analysis. These risk of bias concerns require downgrading even when the finding itself is statistically significant."
        }
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üîç", title: "Challenge 2: The Meta-Analysis Puzzle",
        xp: 35,
        scenario: "A meta-analysis of 20 RCTs (n=15,000) shows significant mortality reduction (RR 0.85, 95% CI 0.76-0.95). However: I¬≤=68%, prediction interval 0.62-1.15, funnel plot shows slight asymmetry, 4 trials registered but never published.",
        question: "Which domains require downgrading and by how much?",
        options: [
          { letter: "A", text: "Inconsistency (‚àí1), publication bias (‚àí1): HIGH ‚Üí LOW", correct: true },
          { letter: "B", text: "Imprecision only (‚àí1): HIGH ‚Üí MODERATE", correct: false },
          { letter: "C", text: "Inconsistency (‚àí2): HIGH ‚Üí LOW", correct: false },
          { letter: "D", text: "Publication bias only (‚àí1): HIGH ‚Üí MODERATE", correct: false }
        ],
        feedback: {
          correct: "Correct! Inconsistency: I¬≤=68% with prediction interval crossing 1.0 (could show harm) warrants one downgrade. Publication bias: 4 registered but unpublished trials plus funnel asymmetry warrants one downgrade. Two domains = two downgrades = HIGH ‚Üí LOW.",
          incorrect: "The prediction interval (0.62-1.15) crosses 1.0‚Äîthe next study might show harm. That's serious inconsistency. The funnel asymmetry plus 4 unpublished registered trials is direct evidence of publication bias. Both domains warrant downgrading."
        }
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üíâ", title: "Challenge 3: The Vaccine Efficacy",
        xp: 35,
        scenario: "A vaccine trial (n=40,000) shows 95% efficacy against symptomatic disease (RR 0.05, 95% CI 0.03-0.08). Low risk of bias, consistent subgroup results, direct outcome. However, follow-up is only 3 months, and durability of protection is unknown.",
        question: "How should certainty be rated for short-term vs. long-term efficacy?",
        options: [
          { letter: "A", text: "HIGH for both‚Äîlarge, well-conducted trial", correct: false },
          { letter: "B", text: "HIGH for short-term; VERY LOW for long-term (no data)", correct: false },
          { letter: "C", text: "HIGH for short-term efficacy; LOW for 12-month efficacy (indirect/imprecise)", correct: true },
          { letter: "D", text: "MODERATE for both‚Äîduration uncertainty affects all conclusions", correct: false }
        ],
        feedback: {
          correct: "Correct! Short-term efficacy: large RCT, low bias, precise, direct = HIGH. Long-term efficacy: either indirect (extrapolating from 3 months) or imprecise (no data). GRADE is outcome-specific‚Äîdifferent time horizons may have different certainty ratings.",
          incorrect: "GRADE rates certainty for specific outcomes. Three-month data directly supports 3-month efficacy (HIGH). Twelve-month efficacy requires extrapolation (indirectness) or has no direct data (imprecision). The same trial supports different certainty for different questions."
        }
      }},
      { type: "decision", theme: "dark", content: {
        icon: "‚öñÔ∏è", title: "Challenge 4: The Observational Upgrade",
        xp: 35,
        scenario: "Cohort studies show a procedure reduces mortality from 95% to 15% (RR 0.16). No RCTs exist because the natural history (95% death) makes randomization unethical. Dose-response is present: earlier intervention shows better outcomes. All plausible confounders would make patients receiving intervention sicker (selection of more severe cases).",
        question: "Should this observational evidence be upgraded, and to what level?",
        options: [
          { letter: "A", text: "No upgrade‚Äîobservational evidence stays LOW", correct: false },
          { letter: "B", text: "Upgrade to MODERATE‚Äîone upgrading factor (large effect)", correct: false },
          { letter: "C", text: "Upgrade to HIGH‚Äîall three upgrading factors clearly present", correct: true },
          { letter: "D", text: "Upgrade to MODERATE‚Äîobservational can never reach HIGH", correct: false }
        ],
        feedback: {
          correct: "Correct! All three upgrading factors: (1) Large effect‚ÄîRR 0.16 (84% relative reduction from 95% to 15%). (2) Dose-response‚Äîearlier = better. (3) Confounding direction‚Äîsicker patients receive intervention, which would bias AGAINST benefit if anything. Multiple upgrading factors can raise observational evidence to HIGH.",
          incorrect: "This meets all three upgrading criteria. Effect size (RR 0.16) is very large‚Äî95% to 15% mortality. Dose-response present. And confounding runs OPPOSITE to the observed effect (sicker patients get the procedure). Observational evidence can reach HIGH certainty when all criteria are clearly met."
        }
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üéØ", title: "Challenge 5: The Composite Outcome",
        xp: 35,
        scenario: "An RCT shows significant reduction in 'major adverse cardiovascular events' (MACE: CV death, MI, stroke). However, decomposition reveals: CV death RR 1.02 (95% CI 0.85-1.22), MI RR 0.82 (0.72-0.94), Stroke RR 0.95 (0.80-1.13). The MACE benefit is driven entirely by MI.",
        question: "How should certainty differ for MACE vs. individual components?",
        options: [
          { letter: "A", text: "Same certainty‚ÄîMACE is the primary outcome", correct: false },
          { letter: "B", text: "MACE HIGH; CV death/stroke may be rated MODERATE (imprecision)", correct: false },
          { letter: "C", text: "MI HIGH; CV death/stroke LOW (CI includes substantial harm/benefit); MACE MODERATE (driven by MI)", correct: true },
          { letter: "D", text: "All components HIGH‚Äîthey're from the same well-conducted RCT", correct: false }
        ],
        feedback: {
          correct: "Correct! GRADE is outcome-specific. MI evidence is precise (0.72-0.94 excludes 1.0). CV death and stroke CIs are wide and cross 1.0‚Äîwe can't exclude no effect or harm. MACE driven entirely by MI means the composite obscures component-specific uncertainty. Different outcomes warrant different ratings even from the same trial.",
          incorrect: "Composite outcomes can mask component-specific uncertainty. The RCT shows clear MI reduction but uncertainty about CV death and stroke. Patients and clinicians need to know whether the drug prevents deaths or just non-fatal events. GRADE rates each outcome separately."
        }
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üìâ", title: "Challenge 6: The Fragile Finding",
        xp: 35,
        scenario: "A meta-analysis of 5 small RCTs (total n=800, 48 events) shows statistically significant mortality benefit (RR 0.55, 95% CI 0.32-0.95, p=0.03). The fragility index is 2. A single large RCT (n=3,000) is underway.",
        question: "What certainty rating is appropriate while awaiting the large trial?",
        options: [
          { letter: "A", text: "HIGH‚Äîstatistically significant benefit from RCTs", correct: false },
          { letter: "B", text: "MODERATE‚Äîsome imprecision but consistent direction", correct: false },
          { letter: "C", text: "LOW‚Äîserious imprecision (48 events, fragility index 2, below OIS)", correct: true },
          { letter: "D", text: "Cannot rate‚Äîwait for the large trial", correct: false }
        ],
        feedback: {
          correct: "Correct! 48 events is far below typical OIS (200-400 for mortality). Fragility index of 2 means 2 different outcomes would flip the result. The CI is wide (0.32-0.95). This is classic imprecision requiring downgrade. We can make provisional conclusions while acknowledging the large trial may change everything.",
          incorrect: "Statistical significance doesn't guarantee precision. The fragility index of 2 means the 'significant' result is extremely unstable. 48 events is grossly underpowered for mortality conclusions. GRADE requires downgrading for imprecision‚Äîwe should be honest about uncertainty while awaiting better data."
        }
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üß¨", title: "Challenge 7: The Surrogate Outcome",
        xp: 35,
        scenario: "A drug dramatically improves bone mineral density (+8% annually). Observational data suggest each 1% density improvement correlates with 5% fracture reduction. No fracture outcome trials have been conducted. The drug class has previously produced agents where density improved but fractures increased.",
        question: "What certainty applies to fracture prevention?",
        options: [
          { letter: "A", text: "MODERATE‚Äîstrong surrogate relationship", correct: false },
          { letter: "B", text: "LOW‚Äîindirect evidence, but plausible mechanism", correct: false },
          { letter: "C", text: "VERY LOW‚Äîserious indirectness (surrogate only) PLUS known discordance in drug class", correct: true },
          { letter: "D", text: "Cannot rate until fracture trials complete", correct: false }
        ],
        feedback: {
          correct: "Correct! Surrogate outcomes require downgrading for indirectness. But here there's added concern: the drug class has precedent for density-fracture discordance (like sodium fluoride). This history of surrogate-outcome discordance in the same class warrants serious downgrading. Starting LOW (observational), downgrade further for known class discordance.",
          incorrect: "The sodium fluoride case (density up, fractures up) is directly relevant‚Äîsame drug class with known surrogate-outcome discordance. When a drug class has shown that the surrogate can move opposite to outcomes, surrogate data from a new drug in that class is deeply uncertain."
        }
      }},
      { type: "decision", theme: "dark", content: {
        icon: "üè•", title: "Challenge 8: The System-Level Evidence",
        xp: 35,
        scenario: "A stepped-wedge cluster RCT shows a hospital protocol reduces 30-day mortality (OR 0.75, 95% CI 0.62-0.91). Twenty hospitals participated. However, the intervention was implemented differently across sites, outcomes were assessed unblinded, and 5 hospitals withdrew mid-trial.",
        question: "Which GRADE domains are affected by these methodological features?",
        options: [
          { letter: "A", text: "Risk of bias only‚Äîother domains unaffected", correct: false },
          { letter: "B", text: "Risk of bias (unblinded, attrition) + inconsistency (implementation variation)", correct: true },
          { letter: "C", text: "Imprecision only‚Äî20 hospitals may be insufficient", correct: false },
          { letter: "D", text: "Indirectness‚Äîcluster trials don't apply to individual patients", correct: false }
        ],
        feedback: {
          correct: "Correct! Risk of bias: unblinded outcome assessment introduces detection bias; 5 hospital withdrawals are attrition bias. Inconsistency: variable implementation across sites may cause heterogeneity (different hospitals implementing different versions). Both domains warrant scrutiny.",
          incorrect: "Unblinded assessment and attrition (5 hospitals dropping out) are classic risk of bias concerns. Variable implementation is an inconsistency concern‚Äîif hospitals implemented differently, the 'intervention' isn't uniform and results may vary by implementation fidelity."
        }
      }},
      { type: "stats", theme: "dark", content: {
        title: "GRADE in Numbers",
        stats: [
          { value: "4", label: "Certainty levels" },
          { value: "5", label: "Downgrading domains" },
          { value: "3", label: "Upgrading factors" },
          { value: "‚àû", label: "Lives protected by rigorous assessment" }
        ],
        caption: "Systematic certainty assessment protects patients from overconfident medicine."
      }},
      { type: "refrain", theme: "black", content: { text: "Certainty is not about how much evidence exists. It's about how much we should trust it." }},
      { type: "quiz", theme: "dark", content: {
        question: "What is the fundamental insight that GRADE provides for evidence-based medicine?",
        options: [
          { text: "Larger studies are always better", correct: false },
          { text: "RCTs are the only valid evidence", correct: false },
          { text: "Certainty must be systematically assessed, with transparent reasoning for each domain", correct: true },
          { text: "Meta-analyses provide definitive answers", correct: false }
        ],
        xp: 25,
        feedback: {
          correct: "Exactly. GRADE transforms implicit judgments into explicit, transparent assessments. By systematically considering risk of bias, inconsistency, indirectness, imprecision, and publication bias‚Äîand documenting reasoning‚ÄîGRADE makes certainty assessment reproducible and debatable. This transparency is its core contribution.",
          incorrect: "GRADE's fundamental contribution is structured transparency. Rather than vague quality labels, it requires systematic assessment of specific domains with explicit reasoning. This makes certainty judgments reproducible, debatable, and improvable as evidence accumulates."
        }
      }}
    ]
  }
];

// ===== NAVIGATION STATE =====
let currentModule = 0;
let currentSlide = 0;
let consecutiveCorrect = gameState.consecutiveCorrect || 0;

// ===== SAVE/LOAD =====
function saveGame() {
  safeSetStorage('gradeGame_v1', gameState);
}

function resetProgress() {
  if (confirm('Reset all progress and achievements?')) {
    gameState = { ...DEFAULT_STATE, answeredQuestions: {}, badges: [], completedModules: [] };
    saveGame();
    currentModule = 0;
    currentSlide = 0;
    renderAll();
  }
}

// ===== COURSE PROGRESS =====
function updateCourseProgress() {
  const totalModules = MODULES.length;
  const completedCount = gameState.completedModules.length;
  const pct = Math.round((completedCount / totalModules) * 100);
  const fillEl = document.getElementById('courseProgressFill');
  const pctEl = document.getElementById('courseProgressPct');
  if (fillEl) fillEl.style.width = pct + '%';
  if (pctEl) pctEl.textContent = pct + '%';
}

// ===== XP & LEVELING =====
function addXP(amount) {
  gameState.xp += amount;
  checkLevelUp();
  saveGame();
  updateStatsDisplay();
  showXPPopup(amount);
}

function checkLevelUp() {
  for (let i = LEVELS.length - 1; i >= 0; i--) {
    if (gameState.xp >= LEVELS[i].xp && gameState.level < LEVELS[i].level) {
      gameState.level = LEVELS[i].level;
      showLevelUp(LEVELS[i]);
      break;
    }
  }
}

function showXPPopup(amount) {
  const popup = document.getElementById('xpPopup');
  document.getElementById('xpAmount').textContent = `+${amount}`;
  popup.classList.add('show');
  setTimeout(() => popup.classList.remove('show'), 1500);
}

function showLevelUp(levelData) {
  triggerConfetti();
}

// ===== BADGES =====
function awardBadge(badgeId) {
  if (gameState.badges.includes(badgeId)) return;
  gameState.badges.push(badgeId);
  saveGame();
  const badge = BADGES.find(b => b.id === badgeId);
  if (badge) showBadgeModal(badge);
  updateBadges();
}

function showBadgeDetails(badgeId) {
  const badge = BADGES.find(b => b.id === badgeId);
  if (badge) showBadgeModal(badge);
}

let _badgeModalTrapHandler = null;

function showBadgeModal(badge) {
  const modal = document.getElementById('badgeModal');
  document.getElementById('badgeModalIcon').textContent = badge.icon;
  document.getElementById('badgeModalTitle').textContent = badge.name;
  document.getElementById('badgeModalDesc').textContent = badge.desc;
  modal.classList.add('show');
  triggerConfetti();

  // Focus trap for accessibility
  const focusableElements = modal.querySelectorAll('button, [tabindex]:not([tabindex="-1"])');
  if (focusableElements.length > 0) {
    focusableElements[0].focus();
  }

  // Remove any previous handler before adding a new one
  if (_badgeModalTrapHandler) {
    modal.removeEventListener('keydown', _badgeModalTrapHandler);
  }

  _badgeModalTrapHandler = function(e) {
    if (e.key === 'Escape') {
      closeBadgeModal();
      return;
    }
    if (e.key === 'Tab') {
      const firstFocusable = focusableElements[0];
      const lastFocusable = focusableElements[focusableElements.length - 1];
      if (e.shiftKey && document.activeElement === firstFocusable) {
        e.preventDefault();
        lastFocusable.focus();
      } else if (!e.shiftKey && document.activeElement === lastFocusable) {
        e.preventDefault();
        firstFocusable.focus();
      }
    }
  };
  modal.addEventListener('keydown', _badgeModalTrapHandler);
}

function closeBadgeModal() {
  const modal = document.getElementById('badgeModal');
  if (_badgeModalTrapHandler) {
    modal.removeEventListener('keydown', _badgeModalTrapHandler);
    _badgeModalTrapHandler = null;
  }
  modal.classList.remove('show');
}

// ===== STREAK =====
function updateStreak() {
  const today = new Date().toDateString();
  if (gameState.lastActive !== today) {
    const yesterday = new Date(Date.now() - 86400000).toDateString();
    if (gameState.lastActive === yesterday) {
      gameState.streak++;
      if (gameState.streak >= 3) awardBadge('streak_3');
      if (gameState.streak >= 7) awardBadge('streak_7');
    } else if (gameState.lastActive) {
      gameState.streak = 1;
    } else {
      gameState.streak = 1;
    }
    gameState.lastActive = today;
    saveGame();
  }
}

// ===== CONFETTI =====
function triggerConfetti() {
  if (window.matchMedia('(prefers-reduced-motion: reduce)').matches) return;
  const container = document.getElementById('confettiContainer');
  const colors = ['#D4AF37', '#22c55e', '#3b82f6', '#f59e0b', '#ef4444', '#7c3aed'];
  for (let i = 0; i < 50; i++) {
    const confetti = document.createElement('div');
    confetti.className = 'confetti';
    confetti.style.left = Math.random() * 100 + '%';
    confetti.style.background = colors[Math.floor(Math.random() * colors.length)];
    confetti.style.animationDelay = Math.random() * 0.5 + 's';
    container.appendChild(confetti);
    setTimeout(() => confetti.remove(), 3500);
  }
}

// ===== COMBO =====
function updateComboDisplay() {
  const comboEl = document.getElementById('comboDisplay');
  const countEl = document.getElementById('comboCount');
  if (consecutiveCorrect >= 2) {
    countEl.textContent = consecutiveCorrect;
    comboEl.classList.add('show');
  } else {
    comboEl.classList.remove('show');
  }
}

// ===== MOBILE SIDEBAR =====
function toggleSidebar() {
  document.querySelector('.sidebar').classList.toggle('open');
  document.getElementById('sidebarOverlay').classList.toggle('show');
  var btn = document.querySelector('.sidebar-toggle, .hamburger, .mobile-menu-btn'); if (btn) btn.setAttribute('aria-expanded', btn.getAttribute('aria-expanded') === 'true' ? 'false' : 'true');
}

// ===== RENDER =====
function renderAll() {
  updateStreak();
  renderSidebar();
  renderSlides();
  updateStatsDisplay();
  updateBadges();
  updateCourseProgress();
}

function updateStatsDisplay() {
  const levelData = LEVELS.find(l => l.level === gameState.level) || LEVELS[0];
  const nextLevel = LEVELS.find(l => l.level === gameState.level + 1);

  document.getElementById('levelIcon').textContent = levelData.icon;
  document.getElementById('levelTitle').textContent = levelData.title;
  document.getElementById('levelNum').textContent = gameState.level;

  const currentXP = gameState.xp - levelData.xp;
  const neededXP = nextLevel ? (nextLevel.xp - levelData.xp) : 500;
  const pct = Math.min(100, (currentXP / neededXP) * 100);

  document.getElementById('xpFill').style.width = pct + '%';
  document.getElementById('xpCurrent').textContent = currentXP;
  document.getElementById('xpNeeded').textContent = neededXP;

  document.getElementById('correctCount').textContent = gameState.correctAnswers;
  document.getElementById('decisionCount').textContent = gameState.decisionsCompleted;
  document.getElementById('gradesCount').textContent = gameState.gradesAssigned || 0;
  document.getElementById('streakCount').textContent = gameState.streak;

  const streakEl = document.getElementById('streakDisplay');
  streakEl.classList.toggle('active', gameState.streak > 0);
}

function updateBadges() {
  const grid = document.getElementById('badgesGrid');
  grid.innerHTML = BADGES.map(b => {
    const earned = gameState.badges.includes(b.id);
    return `<button class="badge-item ${earned ? 'earned' : ''}" type="button" ${earned ? `onclick="showBadgeDetails('${b.id}')"` : ''} aria-label="${b.name}: ${b.desc}" title="${b.name}: ${b.desc}" style="min-width:44px;min-height:44px">${b.icon}</button>`;
  }).join('');
}

function renderSidebar() {
  const list = document.getElementById('moduleList');
  list.innerHTML = MODULES.map((mod, i) => {
    const completed = gameState.completedModules.includes(mod.id);
    const locked = i > 0 && !gameState.completedModules.includes(MODULES[i-1].id);
    const estMins = Math.round(mod.slides.length * 1.5);
    return `
      <div class="module-item ${i === currentModule ? 'active' : ''} ${completed ? 'completed' : ''} ${locked ? 'locked' : ''}"
           onclick="${locked ? '' : `goToModule(${i})`}"
           tabindex="${locked ? -1 : 0}"
           role="button"
           aria-disabled="${locked ? 'true' : 'false'}">
        <div class="module-number">${completed ? '‚úì' : (locked ? 'üîí' : mod.id)}</div>
        <div class="module-info">
          <div class="module-title">${mod.title}</div>
          <div class="module-subtitle">${mod.subtitle}</div>
          <div class="module-xp">+${mod.xp} XP</div>
          <div class="module-time">‚è±Ô∏è ~${estMins} min</div>
        </div>
      </div>
    `;
  }).join('');
}

function renderSlides() {
  const container = document.getElementById('slidesContainer');
  container.innerHTML = MODULES.map((mod, mi) => {
    const progressPct = ((currentSlide + 1) / mod.slides.length) * 100;
    return `
    <div class="module-slides ${mi === currentModule ? 'active' : ''}" data-module="${mi}">
      <div class="slide-progress-bar" style="width: ${mi === currentModule ? progressPct : 0}%"></div>
      ${mod.slides.map((slide, si) => renderSlide(slide, si, mi)).join('')}
      <div class="slide-nav">
        <button class="nav-btn" onclick="prevSlide()" ${currentSlide === 0 ? 'disabled' : ''} aria-label="Previous slide">‚Üê</button>
        <div class="nav-center">
          <div class="slide-counter">${currentSlide + 1} / ${mod.slides.length}</div>
          <div class="progress-dots">
            ${mod.slides.map((_, i) => `<button class="progress-dot ${i === currentSlide ? 'active' : ''}" type="button" onclick="goToSlide(${i})" aria-label="Go to slide ${i + 1}" ${i === currentSlide ? 'aria-current="true"' : ''}></button>`).join('')}
          </div>
        </div>
        <button class="nav-btn" onclick="nextSlide()" aria-label="Next slide">‚Üí</button>
      </div>
      <div class="keyboard-hint" style="position:absolute;bottom:5px;left:50%;transform:translateX(-50%);font-size:0.6rem;opacity:0.4;">‚Üê ‚Üí arrows or swipe to navigate</div>
    </div>
  `;
  }).join('');
  document.getElementById('currentModule').textContent = MODULES[currentModule].title;
  updateSlideVisibility();
}

function renderSlide(slide, index, moduleIndex) {
  const theme = slide.theme || 'dark';
  let html = `<div class="slide ${theme} ${index === currentSlide ? 'active' : ''}" data-slide="${index}">`;

  switch(slide.type) {
    case 'title':
      html += `<div class="content"><h1 class="title gold animate">${slide.content.title}</h1><p class="subtitle animate delay-1">${slide.content.subtitle}</p></div>`;
      break;

    case 'objectives':
      html += `<div class="content">
        <div class="objectives-box animate">
          <div class="objectives-header">
            <div class="objectives-icon">üéØ</div>
            <div class="objectives-title">Learning Objectives</div>
          </div>
          <ul class="objectives-list">
            ${slide.content.objectives.map(obj => `<li>${obj}</li>`).join('')}
          </ul>
        </div>
        ${slide.content.tip ? `<p class="subtitle animate delay-1" style="font-size:0.85rem;max-width:600px">${slide.content.tip}</p>` : ''}
      </div>`;
      break;

    case 'story':
      html += `<div class="content">
        <article class="story-box ${slide.content.success ? 'success' : ''} animate" role="article">
          <div class="story-label">${slide.content.label}</div>
          ${slide.content.title ? `<h2 class="title gold" style="font-size:1.8rem;margin-bottom:0.75rem">${slide.content.title}</h2>` : ''}
          <p class="story-text">${slide.content.text}</p>
          ${slide.content.followup ? `<p class="story-text" style="margin-top:0.75rem;font-style:italic;opacity:0.9">${slide.content.followup}</p>` : ''}
        </article>
      </div>`;
      break;

    case 'stats':
      html += `<div class="content">
        ${slide.content.title ? `<h2 class="title gold animate">${slide.content.title}</h2>` : ''}
        ${slide.content.subtitle ? `<p class="subtitle animate delay-1">${slide.content.subtitle}</p>` : ''}
        <div class="stats-grid animate delay-2">
          ${slide.content.stats.map(s => `<div class="stat-box"><div class="stat-value gold">${s.value}</div><div class="stat-label">${s.label}</div></div>`).join('')}
        </div>
        ${slide.content.caption ? `<p class="subtitle animate delay-3" style="font-size:0.85rem;margin-top:0.75rem">${slide.content.caption}</p>` : ''}
      </div>`;
      break;

    case 'refrain':
      html += `<div class="content" style="justify-content:center;height:100%"><p class="refrain animate">"${slide.content.text}"</p></div>`;
      break;

    case 'concept':
      html += `<div class="content">
        <h2 class="title ${theme === 'light' ? 'navy' : 'gold'} animate">${slide.content.title}</h2>
        ${slide.content.subtitle ? `<p class="subtitle animate delay-1">${slide.content.subtitle}</p>` : ''}
        ${slide.content.text ? `<div class="card ${theme === 'light' ? '' : 'navy-bg'} animate delay-2"><p class="card-text">${slide.content.text}</p></div>` : ''}
        ${slide.content.highlight ? `<div class="story-box warning animate delay-3"><div class="story-label">Key Insight</div><p class="story-text">${slide.content.highlight}</p></div>` : ''}
        ${slide.content.checkpoints ? `
          <div class="timeline animate delay-2" style="margin-top:0.75rem">
            ${slide.content.checkpoints.map(cp => `
              <div class="timeline-item"><div class="timeline-days">${cp.day}</div><div class="timeline-content"><div class="timeline-title">${cp.event}</div></div></div>
            `).join('')}
          </div>
        ` : ''}
      </div>`;
      break;

    case 'checklist':
      html += `<div class="content">
        <h2 class="title gold animate">${slide.content.title}</h2>
        <div class="checklist animate delay-1">
          ${slide.content.items.map(item => `
            <div class="checklist-item">
              <div class="check-icon">‚úì</div>
              <div class="checklist-text">${item}</div>
            </div>
          `).join('')}
        </div>
      </div>`;
      break;

    case 'decision':
      const did = `dec_${moduleIndex}_${index}`;
      const answered = gameState.answeredQuestions[did];
      html += `<div class="content">
        <div class="decision-tree animate">
          <div class="decision-header">
            <div class="decision-icon">${slide.content.icon}</div>
            <div class="decision-title">${slide.content.title}</div>
            <div class="decision-xp">+${slide.content.xp} XP</div>
          </div>
          <div class="decision-scenario">${slide.content.scenario}</div>
          <div class="decision-question">${slide.content.question}</div>
          <div class="decision-options">
            ${slide.content.options.map(opt => {
              const isSelected = answered === opt.letter;
              const showCorrect = answered && opt.correct;
              const showIncorrect = answered && isSelected && !opt.correct;
              return `
                <div class="decision-option ${isSelected ? 'selected' : ''} ${showCorrect ? 'correct' : ''} ${showIncorrect ? 'incorrect' : ''} ${answered ? 'disabled' : ''}"
                     onclick="answerDecision('${did}', '${opt.letter}', ${opt.correct}, ${slide.content.xp})"
                     tabindex="0"
                     role="button"
                     aria-disabled="${answered ? 'true' : 'false'}">
                  <div class="option-letter">${opt.letter}</div>
                  <div class="option-text">${opt.text}</div>
                </div>
              `;
            }).join('')}
          </div>
          <div class="decision-feedback ${answered ? 'show' : ''} ${answered && slide.content.options.find(o => o.letter === answered)?.correct ? 'correct' : 'incorrect'}" role="alert" aria-live="polite">
            <div class="feedback-title">${answered && slide.content.options.find(o => o.letter === answered)?.correct ? '‚úì Correct!' : '‚úó Not quite'}</div>
            <div class="feedback-text">${answered && slide.content.options.find(o => o.letter === answered)?.correct ? slide.content.feedback.correct : slide.content.feedback.incorrect}</div>
          </div>
        </div>
      </div>`;
      break;

    case 'quiz':
      const qid = `quiz_${moduleIndex}_${index}`;
      const qAnswered = gameState.answeredQuestions[qid];
      html += `<div class="content">
        <div class="quiz-container animate">
          <div class="quiz-question">${slide.content.question}</div>
          <div class="quiz-options">
            ${slide.content.options.map((opt, oi) => {
              const isSelected = qAnswered === oi;
              const showCorrect = qAnswered !== undefined && opt.correct;
              const showIncorrect = qAnswered !== undefined && isSelected && !opt.correct;
              return `
                <div class="quiz-option ${showCorrect ? 'correct' : ''} ${showIncorrect ? 'incorrect' : ''} ${qAnswered !== undefined ? 'disabled' : ''}"
                     onclick="answerQuiz('${qid}', ${oi}, ${opt.correct}, ${slide.content.xp})"
                     tabindex="0"
                     role="button">
                  ${opt.text}
                </div>
              `;
            }).join('')}
          </div>
          <div class="quiz-feedback ${qAnswered !== undefined ? 'show' : ''} ${qAnswered !== undefined && slide.content.options[qAnswered]?.correct ? 'correct' : 'incorrect'}" role="alert" aria-live="polite">
            ${qAnswered !== undefined && slide.content.options[qAnswered]?.correct ? slide.content.feedback.correct : slide.content.feedback.incorrect}
          </div>
        </div>
      </div>`;
      break;
  }

  html += '</div>';
  return html;
}

function updateSlideVisibility() {
  const container = document.querySelector('.module-slides.active');
  if (!container) return;

  container.querySelectorAll('.slide').forEach((slide, i) => {
    slide.classList.toggle('active', i === currentSlide);
  });

  const progressBar = container.querySelector('.slide-progress-bar');
  if (progressBar) {
    const pct = ((currentSlide + 1) / MODULES[currentModule].slides.length) * 100;
    progressBar.style.width = pct + '%';
  }

  container.querySelector('.slide-counter').textContent = `${currentSlide + 1} / ${MODULES[currentModule].slides.length}`;
  container.querySelectorAll('.progress-dot').forEach((dot, i) => {
    dot.classList.toggle('active', i === currentSlide);
  });
  container.querySelector('.nav-btn').disabled = currentSlide === 0;
}

// ===== ANSWERS =====
function answerDecision(did, letter, correct, xp) {
  if (gameState.answeredQuestions[did] !== undefined) return;
  gameState.answeredQuestions[did] = letter;
  gameState.decisionsCompleted++;
  gameState.gradesAssigned = (gameState.gradesAssigned || 0) + 1;

  // Award first_grade badge on first decision
  if (gameState.decisionsCompleted === 1) awardBadge('first_grade');

  if (correct) {
    gameState.correctAnswers++;
    consecutiveCorrect++;
    addXP(xp);
    if (consecutiveCorrect >= 5) awardBadge('perfect_5');
    if (gameState.decisionsCompleted >= 15) awardBadge('decision_maker');
  } else {
    consecutiveCorrect = 0;
  }
  gameState.consecutiveCorrect = consecutiveCorrect;
  saveGame();
  renderSlides();
  updateComboDisplay();
  updateStatsDisplay();
}

function answerQuiz(qid, choice, correct, xp) {
  if (gameState.answeredQuestions[qid] !== undefined) return;
  gameState.answeredQuestions[qid] = choice;

  if (correct) {
    gameState.correctAnswers++;
    consecutiveCorrect++;
    addXP(xp);
    if (consecutiveCorrect >= 5) awardBadge('perfect_5');
  } else {
    consecutiveCorrect = 0;
  }
  gameState.consecutiveCorrect = consecutiveCorrect;
  saveGame();
  renderSlides();
  updateComboDisplay();
}

// ===== NAVIGATION =====
function goToModule(index) {
  if (index > 0 && !gameState.completedModules.includes(MODULES[index-1].id)) return;
  currentModule = index;
  currentSlide = 0;
  renderSlides();
  renderSidebar();
}

function goToSlide(index) {
  currentSlide = index;
  updateSlideVisibility();
}

function nextSlide() {
  const mod = MODULES[currentModule];
  if (currentSlide < mod.slides.length - 1) {
    currentSlide++;
    updateSlideVisibility();
  } else {
    completeModule();
  }
}

function prevSlide() {
  if (currentSlide > 0) {
    currentSlide--;
    updateSlideVisibility();
  }
}

function completeModule() {
  const mod = MODULES[currentModule];
  if (!gameState.completedModules.includes(mod.id)) {
    gameState.completedModules.push(mod.id);
    addXP(mod.xp);

    // Module-specific badges
    if (mod.id === 3) awardBadge('bias_hunter');
    if (mod.id === 4) awardBadge('consistency_king');
    if (mod.id === 5) awardBadge('directness_detective');
    if (mod.id === 6) awardBadge('precision_master');
    if (mod.id === 7) awardBadge('publication_sleuth');
    if (mod.id === 8) awardBadge('upgrader');
    if (mod.id === 10) awardBadge('grade_master');
    if (gameState.completedModules.length === MODULES.length) awardBadge('finisher');

    saveGame();
    triggerConfetti();
    updateCourseProgress();
  }

  if (currentModule < MODULES.length - 1) {
    goToModule(currentModule + 1);
  }
}

// ===== KEYBOARD =====
document.addEventListener('keydown', (e) => {
  const isRoleButton = e.target && e.target.getAttribute && e.target.getAttribute('role') === 'button';
  const isFormControl = e.target && e.target.closest && e.target.closest('button, a, input, textarea, select, [role="radio"], [role="option"]');
  if (isRoleButton || isFormControl) {
    if ((e.key === ' ' || e.key === 'Enter') && isRoleButton) { e.preventDefault(); e.target.click(); }
    return;
  }
  if (e.key === 'Escape') {
    closeBadgeModal();
    document.querySelector('.sidebar').classList.remove('open');
    document.getElementById('sidebarOverlay').classList.remove('show');
    return;
  }
  if (e.key === 'ArrowRight' || e.key === ' ') { e.preventDefault(); nextSlide(); }
  else if (e.key === 'ArrowLeft') { e.preventDefault(); prevSlide(); }
});

// Second keydown handler removed (first handler covers role="button" elements)

// ===== TOUCH =====
let touchStartX = 0;
document.addEventListener('touchstart', (e) => { touchStartX = e.changedTouches[0].screenX; }, { passive: true });
document.addEventListener('touchend', (e) => {
  const diff = touchStartX - e.changedTouches[0].screenX;
  if (Math.abs(diff) > 50) { diff > 0 ? nextSlide() : prevSlide(); }
}, { passive: true });

// ===== INIT =====
renderAll();
</script>
</body>
</html>
